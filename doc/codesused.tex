\chapter{Codes utilized - Identyfing voids and filaments from a particle distribution}\label{sec:codesused}
\section{DisPerSE-"Discrete Persistent Structures Extractor"}
For the analysis conducted on filaments, the identification of the filamentary structure of the cosmic web in
data sets is crucial. To achieve this, the code
DisPerSE\cite{2011MNRAS.414..350S}\cite{2011MNRAS.414..384S} is utilized. The
code is publicly available at \url{http://www2.iap.fr/users/sousbie/web/html/indexd41d.html?}.
DisPerSE is a code that was developed to identify topological features of the
cosmic web in cosmological particle distributions. DisPerSE utilizes discrete
Morse Theory to identify the topological structures of the cosmic web in data
sets. In this section, I will give a brief overview of how DisPerSE works to
identify the filamentary structure from a particle distribution. For a thorough
review I recommend the paper \cite{2011MNRAS.414..350S}, in which the following
chapter is based and much notation is borrowed, describing the theory behind DisPerSE and the
references therein. Another good summary of the beforementioned paper is found in \cite{Alexhothesis}, from which the following chapter also has drawn information.
\subsection{Morse Theory}
To identify the features of the cosmic web from the density field of our
particle distribution, DisPerSE relies on Morse Theory \cite{Morse}. Morse
theory is a mathematical framework used to relate the geometrical and
topological properties of a function. In our case, the topological features are quantities such as the number of galaxy clusters or dark matter halos, while the geometric features are quantities related to the precise position of the galaxy cluster or dark matter halos and the local density. In this thesis, Morse theory is utilized to
study the relation between the density field derived from a dark matter halo catalogue and features such as
filaments, voids and walls.
This is done by studying the gradient of a smooth function $f$. By studying the
gradient of the function $f$, one can analyze the topological and geometrical features of
the manifold. In general, to apply Morse theory, one needs a smooth scalar function $f$
that is twice differentiable. In our case, this scalar function is the density
field of our simulation volume. In Morse theory, the gradient of the function $f$
is defined as $\nabla_xf(\vec{x})=df(\vec{x})/d\vec{x}$. This function
specifies the direction of the steepest ascent for our scalar density function. The
points where $\nabla_xf=0$, are called critical points. These critical
points can be classified by studying the Hessian matrix of $f$ given as
\begin{equation}
    \mathcal{H}_f(\vec{x})=d^2f(\vec{x})/dx_idx_j.
\end{equation}
The critical points are classified by the eigenvalues of $\mathcal{H}_f(\vec{x})$. A critical point is of order $k$ when the Hessian matrix has exactly $k$
negative eigenvalues. This way one can classify the critical points as maxima
with $k=2$, saddle point with $k=1$ and minima with $k=0$. This analysis also
implies that for Morse theory to be applicable, the function $f$ has to satisfy
the condition $\mathcal{H}_f(\vec{x})\neq 0$, where $\nabla_xf=0$. From this, it
follows that any function that satisfies this condition can be classified as a Morse
function.\\
 
It is previously stated that the gradient points in a preferred direction for any non-critical
point. From the gradient, one can therefore define integral lines between the critical
points. An integral line is a parametrised curve $L(t)\in\mathbb{R}^d$ that satisfies
\begin{equation}
    \frac{dL(t)}{dt}=\nabla_xf(\vec{x}).
\end{equation}
These integral lines will always have critical points as their origin and
destination. An important property of these integral lines is that they cover
all of $\mathbb{R}^d$, but they never intersect each other. They can however
share the same origin or destination. With these integral lines, one can define
ascending and descending n-manifolds. If one considers a critical point $P$ of
order $k$ on the Morse function living in $\mathbb{R}^d$, an ascending manifold of
$d-k$ dimensions is constructed by the set of points reached by
all integral lines with origin at the critical point $P$. A descending
manifold is a $k-$dimensional region of space defined by all integral lines with their
destination at point $P$. An ascending $3-$manifold associated with minimas trace underdense regions represented by voids. Ascending $2-$manifolds trace walls and the filaments themselves are traced by ascending $1-$manifolds. This leads to the definition of the Morse complex.
The Morse complex is simply the set of all the ascending or descending
manifolds. The Morse complex establishes the link between the geometrical properties such as the location of critical points and the paths of the gradients with the topological feature such as how the critical points are connected and how many there are of each type.\\
\subsection{Discrete Morse theory}
The formality currently introduced deals with smooth continuous functions. This
however is rarely applicable to measured data or simulations. For example, the density field
of our simulation volume is derived from a discrete set of
point particles. Therefore discrete
Morse theory\cite{FORMAN199890} is applied in order to utilize Morse theory on
discrete data. Instead of working on smooth functions, discrete Morse theory is
applied to what is called a simplicial complex. A simplicial complex is a space made up
of simplices. A simplex is a generalization of
a triangle to an arbitrary number of dimensions. For example a k-dimensional
simplex, referred to as a k-simplex, is represented as
\begin{itemize}
    \item a $0$-simplex is a point
    \item a $1$-simplex is a line segment
    \item a $2$-simplex is a triangle
    \item a $3$-simplex is a tetrahedron/pyramid etc.
\end{itemize}
A k-simplex will be denoted as $\sigma_k$ defined by the set of points
$\sigma_k=\{p_0, \dots,p_k\}$. Similarly, one can define the face of a simplex
to be a subset of the original k-simplex. We can define the face as an
l-simplex $\gamma_l=\{p_0,\dots,p_l\}$ with $l\leq k$. If $\gamma_l$ is a face
of $\sigma_k$, then $\sigma_k$ is called a coface of $\gamma_l$. When $k$ and
$l$ only differs by one, a face is called a facet and a coface is called a cofacet. These simplices make up
what is called a simplicial complex in which a discrete Morse function is
defined and discrete Morse theory is applied. A simplicial complex $K$ is made
up of a finite set of simplices such that $\sigma_k\in K$. With this one can
define a discrete Morse function $f$ as a function that maps a real value
$f(\sigma_k)$ for each simplex in the simplicial complex $K$. As for the
Morse function defined in regular Morse theory, the discrete counterpart also has
to be differentiable and the gradient can only flow towards one preferred direction
locally. For this to occur and a Morse function to be defined on the simplicial
complex it has to satisfy the following criteria
\begin{itemize}
    \item there exists at most one facet $\xi_{k-1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\leq f(\xi_{k-1})$, and
    \item there exists at most one cofacet $\chi_{k+1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\geq f(\chi_{k+1})$.
\end{itemize}
These criteria states that locally a simplex has a lower value than its facet and
a higher value than its cofacet. On a discrete Morse function, one defines a
gradient field. This is done by comparing a k-simplex with its facets or
cofacets. One can couple a k-simplex with its facets or cofacet and identify a
preferential flow direction through gradient pairs
\begin{itemize}
    \item If a simplex $\sigma_k$ has exactly one lower valued cofacet $\chi_{k+1}$, then
          $\{\sigma_k,\chi_{k+1}\}$ defines a gradient pair
    \item If a simplex $\sigma_k$ has exactly one higher valued facet $\xi_{k-1}$, then
          $\{\xi_{k-1},\sigma_{k}\}$ defines a gradient pair.
\end{itemize}
Gradient pairs define gradient arrows and define the preferred flow direction in
the discrete vector field. As for the regular Morse theory, the discrete
counterpart also requires an analogy to a critical point. These are called
critical k-simplices. A k-simplex $\sigma_k$ is critical if the following
critera are satisfied
\begin{itemize}
    \item there exists no facet $\xi_{k-1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\leq f(\xi_{k-1})$, and
    \item there exists no cofacet $\chi_{k+1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\geq f(\chi_{k+1})$.
\end{itemize}
In regular Morse theory, the integral lines were crucial in representing the
manifolds in which the Morse complex is defined. Their discrete counterpart
is what is called a V-path. If one considers k-simplexes $\alpha^i_k$ and
$(k+1)$ simplexes $\beta^j_{k+1}$, where $\alpha^{i+1}_k$ is a facet of $\beta^i_{k+1}$, a
V-path is a strictly decreasing an alternating sequence such that
\begin{equation}
    \alpha^0_k, \beta^0_{k+1},\alpha^1_k,\beta^1_{k+1},\dots,\alpha^n_k,\alpha^n_{k+1}.
\end{equation}
Each pair $\{\alpha^i_k,\beta^i_{k+1}\}$ forms a gradient pair. A V-path is a set
of all continuous gradient pairs through the simplicial complex $K$.
With V-paths one can define discrete n-manifolds that make up the discrete
Morse complex. A discrete n-manifold is the set of k-simplices that belongs to a
V-path with either origin or destination at the critical simplex $\sigma_k$.
The n-manifolds are ascending if their origin is $\sigma_k$. Likewise,
the n-manifolds are descending if their destination is $\sigma_k$. These
manifolds make up what is called the discrete Morse complex.

\subsection{Delaunay Tesselation Field Estimator}
In order to compute a density field from a set of point particles, in which one
can compute the discrete Morse complex, DisPerSE utilizes
Delaunay-tesselation\cite{2000A&A...363L..29S}. The Delaunay Tesselation Field
Estimator (DTFE) uses Delaunay triangulation. The Delaunay triangulation
divides the volume containing the particles into a set of triangles in 2D or
tetrahedrons 3D. The volume is divided such that no particle resides inside the
circumcircle of the assigned triangles. This triangulation captures the density of the field in such a
way that if the triangles are small, there are many particles close to each other
and the density is high. Likewise, larger triangles are made up of a point
distribution with a larger distance between the particles and the density is
lower. After assigning a density to each triangle, one can use linear
interpolation, assuming the density changes linearly between each bin, to assign
a density to all bins in the assigned computational grid.

\subsection{Topological Persistence}\label{sec:persistence}
When identifying the structures of the cosmic web from a density field derived
from a point distribution, one may
encounter noise. Structures not supposed to be significant may be interpreted as
so and provide noise for later analysis. To deal with noise DisPerSE utilizes
persistence theory \cite{persistence}. Persistence theory studies the evolution
of what is called excursion sets. An excursion set, or a sub-level set, is given as
\begin{equation}
    (x_1,\dots, x_n)\vert\rho(x_1,\dots, x_n)\geq\rho_0.
\end{equation}
This is the set of points $(x_1,\dots, x_n)$ where $\rho(x_1,\dots, x_n)$ is
larger than some threshold value $\rho_0$. Persistence theory measures the so-called
lifetime of the excursion set. This is a measure of the absolute
difference in value between a pair of two critical points. By cycling through
values for $\rho_0$ one may measure the lifetime of the topological features.
The summary of DisPerSE presented by \cite{Alexhothesis} provides an analogy for this using
an example with a function represented as a landscape submerged under water. I will borrow this analogy here.
One can imagine a function of one variable $\rho(x)$ with peaks and throughs as
mountains submerged under a water level set by $\rho_0$. This is illustrated in
figure \ref{fig:persistence}.
When the whole function is submerged under water the excursion set is empty. As
the water level $\rho_0$ decrease the first highest peak of the function will
eventually become visible and a new component of the excursion set will start to
grow. Eventually the second peak will appear and a new independent component
will start to grow. These two components are independent of each other because,
as seen from above the surface of the water, they are still separated. These two
components will grow until the through separating the two peaks will become
visible. If the persistence of the pair is lower than a given threshold the two independent components of the excursion set will now merge
together and form a persistence pair. When two components merge one is
destroyed. In this case the lowest maximum is destroyed by the minimum as is
illustrated in figure \ref{fig:persistence}.\\

DisPerSE cancels noise in the dataset by computing the persistence pairs
in the discrete Morse complex. Given a persistence pair composed of two
simplicial complexes $q_k=\{\sigma_k,\sigma_{k+1}\}$ on the discrete Morse
function $f$, a persistence ratio $r$ is given as
\begin{equation}
    r(q_k)=\frac{f(\sigma_{k+1})}{f(\sigma_k)}.
\end{equation}
This ratio is used for determining the statiscical significance of persistance
pairs. The significance given in "number of sigmas" reads 
\begin{equation}
    S(q_k)=\mathrm{Erf}^{-1}\Big[\frac{P_k(r(q_k))+1}{2}\Big],
\end{equation}
where Erf is the error function and $P_k(r(q_k))$ is the cumulative probability
that a persistence pair with persistence ratio $r\geq r_0$ exists in the
dataset. The code takes a $\sigma$ input for specifying a threshold for cutting
persistence pairs. The argument specifies the persistence threshold in number of
sigmas and any persistence pair with a probability lower than the given sigma
threshold will be cancelled.


\begin{figure}
   \includegraphics[scale=0.6, trim={0 14cm 0 0},clip]{persistence.png}
   \caption{Example on how DisPerSE uses persistence pairs to simplify topological features. On the left is the unmodified function where the two leftmost critical points forms a persistence pair. The value of the minima is increased while the value of the maxima is increased. This is done untill the value of the minima surpasses that of the maximum and thus the persistence pair is cancelled. This effectively smoothes out features of the function.}
   \label{fig:persistence}
\end{figure}

\section{REVOLVER-REal-space VOid Locations from surVEy Reconstruction}
For much of the void analysis conducted in this thesis the code
REVOLVER-REal-space VOid Locations from surVEy Reconstruction is utilized. This
code is publicly available at \url{https://github.com/seshnadathur/Revolver}.
In the scope of this project the code provides an important utility for the
analysis. It provides code for finding voids in a galaxy or halo catalogue. It also has the ability
to reconstruct real space positions for redshift space positions.
\subsection{ZOBOV-ZOnes BOrdering on Voidness}
The void finding method provided by REVOLVER utilized in this project is based
on the ZOBOV-ZOnes BOrdering on Voidness algorithm . The ZOBOV algorithm was
first presented in \cite{Neyrinck_2008}, in which this subsection is based on. In
order to assign a density to the grid from the point particle distribution ZOBOV
takes use of Voronoi-tesselation. Voronoi tesselation divides the space into
individual cells where each cell belongs to a single particle from the point
distribution. Each cell is divided so that a cell around particle $i$ is made up of all
points in space closer to that particle than any other particle. One can imagine
individual bubbles at each particle in the distribution expanding at an equal
rate. Where these bubbles collide defines the edges of each Voronoi cell. The
density of each cell is then $\rho_i=1/V_i$, where index subscript $i$ represents the
density and volume at each cell respectively.\\

After density has been assigned to
the grid through Voronoi tesselation the particles are divided into zones. The
zones are divided around each density minimum. The center of each zone is a
Voronoi cell with a density lower than all of its neighbors. These centers are
found by tracing each particles neighbors until one reaches such a particle with
density lower than all its neighbors. A zone is defined as the set of particles in
which the density flows downwards towards the minimum. Due to noise present in the
dataset, multiple zones may make up one void. Therefore some of them may need to
be joined together. The way zones are joined is considering, for a 2D density
field and imagining the density function as a scalar function representing a
landscape, individual zones and imagine filling them with water. For each zone
the water level is set to the density at the core of each zone. Gradually
raising the water level other zones will get filled. The process stops when a
zone with a lower core density gets filled, or, if the current zone is the one
with the lowest density, when the whole field is filled with water. The water
level in which the water flows into a deeper zone is recorded as $\rho(z)$, where $z$ denotes a given zone. This will lead to
a very large void (the zone with the lowest density at the center), and many
sub-voids. Now one has to define the edges of the voids to determine which zones
should be counted as voids.
\\\indent
One method is to determine the statistical significance of voids. The
probability of wether a zone should be considered as real is determined by the
ratio of the density contrast of a zone with the minimum density of the zone. By
comparing this to a Poisson particle distribution one can get the estimate for a
likelihood function $P$ which determines the probability that a void with a
given density contrast could arise from Poisson noise. One can then choose to
cut all voids exceeding a given significance level from the dataset.
\subsection{Redshift space reconstruction}\label{sec:reconstruction}
Due to all observational data being measured in redshift space, in order to study
the real space positions of observed quantities one has to apply reconstruction. REVOLVER contains a numerical implementation of
the algorithm described in \cite{Nadathur_2018} and \cite{Burden_reconstruction}, in which describes the details
of how REVOLVER reconstructs real space positions from redshift space. I will give a
short recap of this method here.\\\indent
The algorithm of reconstruction is derived in the framework of Lagrangian
perturbation theory. In this framework the Eularian position $\vec{x}(t)$ is
given as
\begin{equation}
    \vec{x}(t)=\vec{q}+\vec{\Psi}(\vec{q},t),
\end{equation}
where $\vec{q}$ is the initial Lagrangian position and $\vec{\Psi}(t)$ is the
displacement field. The Eularian position separates itself from the Lagrangian
position in the sense that the Eularian position is a specific point in space
while the Lagrangian position is the position of a parcel where the observer
follows that individual parcel with the velocity field through space and time.
To first order, the displacement field $\Psi$ can be modelled together with the
overdensity as $\nabla\Psi^{(1)}(\vec{q},t)=-\delta(\vec{x,t})$, where
$\delta(\vec{x},t)$ is the usual overdensity parameter given in equation \ref{eq:overdensity}.
In order to extract the displacement field from $\vec{\Psi}(\vec{q},t)$ from a
redshifted density distribution, one has to solve \cite{recondisplace}
\begin{equation}\label{eq:reconeq}
    \nabla\cdot\vec{\Psi}+\beta\nabla\cdot(\vec{\Psi}\cdot\hat{r})\hat{r}=-\frac{\delta_g}{b},
\end{equation}
where $\beta$ and the bias $b$ is defined in equations
\ref{eq:beta} and \ref{eq:bias}. The vector $\hat{r}$ is the unit vector in the radial direction. Using a Helmholtz decomposition, one can
express $(\vec{\Psi}\cdot\hat{r})\hat{r}$ as a divergence and a curl component, giving $(\vec{\Psi}\cdot\hat{r})\hat{r}=\nabla A + \nabla \times \vec{B}$.
By approximating $(\vec{\Psi}\cdot\hat{r})\hat{r}$ to be irrotational one can neglect
the curl component.
Using this and rewriting $\vec{\Psi}$ as the gradient of a
potential $\vec{\Psi}=\nabla\phi$, one can rewrite equation \ref{eq:reconeq} as
\begin{equation}
    \nabla^2(\phi+\beta A)=-\frac{\delta_g}{b}.
\end{equation}
This equation can be transformed to Fourier space allowing us to write
\begin{equation}
    \hat{\phi}+\beta\hat{A}=\frac{1}{k^2}\frac{\hat{\delta_g}}{b}.
\end{equation}
In this equation, $\hat{\phi}$, $\hat{A}$ and $\hat{\delta_g}$ denotes the Fourier transform of the quantities.
Remembering that $\vec{\Psi}=\nabla\phi$, one can multiply by $i\vec{k}$ and get
\begin{equation}\label{eq:recontempeq}
    \hat{\Psi}+i\vec{k}\hat{A}=\frac{1}{k^2}i\vec{k}\frac{\hat{\delta_g}}{b}.
\end{equation}
The previous assumption that $(\vec{\Psi}\cdot\hat{r})\hat{r}$ is irrotational
allows us to write $(\vec{\Psi}\cdot\hat{r})\hat{r}\approx\nabla A$. By taking the inverse Fourier transform of equation \ref{eq:recontempeq},
one can aprroximate equation \ref{eq:reconeq} as 
\begin{equation}
    \vec{\Psi}+(\vec{\Psi}\cdot\hat{r})\hat{r}=-\nabla\nabla^{-2}\frac{\delta_g}{b}.
\end{equation}
Using fast Fourier transforms this equation can be solved as \cite{Burden_reconstruction}
\begin{equation}\label{eq:revolversolve}
    \vec{\Psi}=\mathrm{IFFT}\Big[-\frac{i\vec{k}\delta(\vec{k})}{k^2b}\Big]-\frac{f}{1+f}\Big[\mathrm{IFFT}\Big[-\frac{i\vec{k}\delta(\vec{k})}{k^2b}\Big]\cdot\hat{r}\Big]\hat{r}.
\end{equation}
Due to redshift space distortions only appearing along the line of sight direction, the component of displacement field responsible for redshift space distortions is given by $\vec{\Psi}_{RSD}=-f(\vec{\Psi}\cdot\hat{r})\hat{r}$.
\\\indent
REVOLVER solves equation \ref{eq:revolversolve} using fast Fourier transforms.
Using a Voronoi tesselation for estimating the density field $\delta(\vec{x})$ of
the simulation volume, this field is then transformed to Fourier space as
$\delta(\vec{k})$. By then dividing by $bk^2$ and multiplying by $i\vec{k}$, one
will retrieve the expression inside the braces in the equation above. The
inverse Fourier transform is then taken.
