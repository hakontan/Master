\chapter{Codes utilized}
\section{DisPerSE-"Discrete Persistent Structures Extractor"}
In this work identification of the filamentary structure of the cosmic web in
data sets is crucial. To achieve this the code
DisPerSE\cite{2011MNRAS.414..350S}\cite{2011MNRAS.414..384S} is utilized. The
code is publicly available at \url{http://www2.iap.fr/users/sousbie/web/html/indexd41d.html?}.
DisPerSE is a code that was developed to identify topological features of the
cosmic web in cosmological particle distributions. DisPerSE utilized discrete
Morse Theory to identify the topological structures of the cosmic web in data
sets. In this section i will give a brief overview of how DisPerSE works to
identify the filamentary structure from a particle distribution. For a thorough
review i recommend the paper \cite{2011MNRAS.414..350S}, in which the following
chapter is based and much notation is borrowed, describing the theory behind DisPerSE and the
references therein.
\subsection{Morse-Theory}
To identify the features of the cosmic web from the density field of our
particle distribution, DisPerSE relies on Morse Theory \cite{Morse}. Morse
theory is a mathematical framework used to relate the geometrical and
topological properties of a function. In our case we utilize morse theory to
study the relation between the density field and topological features such as
filaments, voids and walls.
This is done by studying the gradient of a smooth function $f$. By studying the
gradient of the said function $f$, one can analyze the topological features of
the manifold. In general, to apply Morse theory one needs a smooth scalar function $f$
that is twice differentiable. In our case this scalar function is the density
field of our simulation volume. In Morse theory the gradient of the function $f$
is defined as $\nabla_xf(\vec{x})=df(\vec{x})/d\vec{x}$. This function
specifies the direction of steepest ascent for our scalar density function. The
points where $\nabla_xf=0$ are called critical points. These critical
points can be classified by studying the Hessian matrix of $f$ given as
\begin{equation}
    \mathcal{H}_f(\vec{x})=d^2f(\vec{x})/dx_idx_j.
\end{equation}
The critical points are classified by studying the eigenvalues of the Hessian
matrix. A critical point is of order $k$ when the Hessian matrix has exactly $k$
negative eigenvalues. This way one can classify the critical points as maxima
with $k=2$, saddle point with $k=1$ and a minima with $k=0$. This analysis also
implies that for morse theory to be applicable, the function $f$ has to satisfy
the condition $\mathcal{H}_f(\vec{x})\neq 0$ where $\nabla_xf=0$. From this it
follows that any function that satisfies this condition is called a Morse
function.\\
 
It is previously stated that the gradient points in a preferred direction for any non-critical
point. From the gradient one can therefore define integral lines between the critical
points. An integral lines is a parametrised curve $L(t)\in\mathbb{R}^n$ that satisfies
\begin{equation}
    \frac{dL(t)}{dt}=\nabla_xf(\vec{x}).
\end{equation}
These integral lines will always have critical points as their origin and
destination. An important property of these integral lines is that they cover
all of $\mathbb{R}^d$, but they never intersect eachother. They can however
share the same origin or destination. With these integral lines one can define
ascending and descending n-manifolds. If one consider a critical point $P$ of
order $k$ on the Morse function living in $\mathbb{R}^d$, we have an ascending manifold of
$d-k$ dimensions. This ascending manifold is constructed by the set of points reached by
all integral lines with origin at the critical point $P$. The descending
manifold is a $k-$dimensional region of space defined by all integral lines with
destination at point $P$. This leads to the definition of the Morse-complex.
The Morse complex is simply the set of all the ascending or descending
manifolds.\\
\subsection{Discrete Morse-theory}
The formality currently introduced deals with smooth continuous functions. This
however is rarely applicable to measured data or simulations. For an example the density field
of our simulation volume is derived from a discrete set of
point particles. Therefore discrete
Morse-theory\cite{FORMAN199890} is applied as to utilize Morse-theory on
discrete data. Instead of working on smooth functions discrete Morse-theory is
applied to what is called a simplicial complex. A simplicial complex is a space made up
of simplices. A simplex is a generalization of
a triangle to an arbitrary number of dimensions. For example a k-dimensional
simplex, reffered to as k-simplex, is represented as
\begin{itemize}
    \item a $0$-simplex is a point
    \item a $1$-simplex is a line segment
    \item a $2$-simplex is a triangle
    \item a $3$-simplex is a tetrahedron/pyramid etc.
\end{itemize}
A k-simplex will be denoted as $\sigma_k$ defined by the set of points
$\sigma_k=\{p_0, \dots,p_k\}$. Similarily one can define the face of a simplex
to be a subset of the original k-simplex. We can define the face as an
l-simplex $\gamma_l=\{p_0,\dots,p_l\}$ with $l\leq k$. If $\gamma_l$ is a face
of $\sigma_k$, then $\sigma_k$ is called a coface of $\gamma_l$. When $k$ and
$l$ only differs by one, a face is called a facet and a coface is called a cofacet. These simplices make up
what is called a simplicial complex in which a discrete Morse-function is
defined and discrete Morse-theory is applied. A simplicial complex $K$ are made
up of a finite set of simplices such that $\sigma_k\in K$. With this one can
define a discrete Morse-function $f$ as a function that maps a real value
$f(\sigma_k)$ for each simplex in the simplicial complex $K$. As for the
Morse-function defined in. regular Morse-theory, the discrete counterpart also has
to be differentiable and the gradient can only flow towards one preferred direction
locally. For this to occur and a Morse-function to be defined on the simplicial
complex it has to satisfy the following criteria
\begin{itemize}
    \item there exists at most one facet $\xi_{k-1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\leq f(\xi_{k-1})$, and
    \item there exists at most one cofacet $\chi_{k+1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\geq f(\chi_{k+1})$.
\end{itemize}
This criteria states that locally a simplex has a lower value than its facet and
a higher value than its cofacet. On a discrete morse function one defines a
gradient field. This is done by comparing a K-simplex with its facets or
cofacets. One can couple a K-simplex with its facets or cofacet and identify a
preferential flow direction through gradient pairs
\begin{itemize}
    \item If a simplex $\sigma_k$ has exactly one lower valued cofacet $\chi_{k+1}$, then
          $\{\sigma_k,\chi_{k+1}\}$ defines a gradient pair
    \item If a simplex $\sigma_k$ has exactly one higher valued facet $\xi_{k-1}$, then
          $\{\xi_{k-1},\sigma_{k}\}$ defines a gradient pair.
\end{itemize}
Gradient pairs define gradient arrows and define the preferred flow direction in
the discrete vector field. As for the regular Morse-theory, the discrete
counterpart also requires an analogy to a critical point. These are called
critical k-simplices. A k-simplex $\sigma_k$ is critical if the following
critera are satisfied
\begin{itemize}
    \item there exists no facet $\xi_{k-1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\leq f(\xi_{k-1})$, and
    \item there exists no cofacet $\chi_{k+1}$ of $\sigma_{k}$ such
    that $f(\sigma_k)\geq f(\chi_{k+1})$.
\end{itemize}
In regular morse theory the integral lines were crucial in representing the
manifolds in which the morse complex is defined. Their discrete counterpart
is what is called a V-path. If one considers k-simplexes $\alpha^i_k$ and
$(k+1)$ simplexes $\beta^j_{k+1}$, where $\alpha a^{i+1}_k$ is a facet of $\beta^i_{k+1}$, a
V-path is a strictly decreasing an alternating sequence such that
\begin{equation}
    \alpha^0_k, \beta^0_{k+1},\alpha^1_k,\beta^1_{k+1},\dots,\alpha^n_k,\alpha^n_{k+1}.
\end{equation}
Each pair ${\alpha^i_k,\beta^i_{k+1}}$ forms a gradient pair. A V-path is a set
of all continuous gradient pairs through the simplicials complex $K$.
With V-paths one can define discrete n-manifolds that make up the discrete
morse-complex. A discrete n-manifold is the set of k-simplices that belongs to a
V-path with either origin or destination at the critical simplex $\sigma_k$.
The n-manifolds are either ascending if their origin is $\sigma_k$. Likewise,
the n-manifolds are descending if ther destionation is $\sigma_k$. These
manifolds make up what is called the discrete Morse-complex.

\subsection{Delaunay Tesselation Field Estimator}
In order to compute a density field from a set of point particles, in which one
can compute the discrete Morse-complex, DisPerSE utilizes
Delaunay-tesselation\cite{2000A&A...363L..29S}. The Delaunay Tesselation Field
Estimator (DTFE) uses Delaunay triangulation. The Delaunay triangulation
divides the volume containing the particles into a set of triangles in 2D or
tetrahedrons 3D. The volume is divided such that no particle resides inside the
circumcircle of the assigned triangles. The vertices of the triangles are made
up of the point distribution and no particles reside inside the area or volume
of the triangles. This triangulation captures the density of the field in such a
way that if the triangles are small there are many particles close to eachother
and the density is high. Likewise larger triangles are made up of a point
distribution with a larger distance between the particles and the density is
lower. After assigning a density to each triangle, one can use linear
interpolation, assuming the density changes linearly between each bin, to assign
a density to all bins in the assigned computational grid.

\subsection{Topological Persistence}\label{sec:persistence}
When identifying the structures of the cosmic web from a density field derived
from a point distribution, one may
encounter noise. Structures not supposed to be significant may be inerpreted as
so and provide noise for later analysis. To deal with noise DisPerSE utilzies
persistence theory \cite{persistence}. Persistence theory studies the evolution
of what is called excursion sets. An excursion set, or a sub level set, is given as
\begin{equation}
    (x_1,\dots, x_n)\vert\rho(x_1,\dots, x_n)\geq\rho_0.
\end{equation}
This is the set of points $(x_1,\dots, x_n)$ where $\rho(x_1,\dots, x_n)$ is
larger than some threshold value $\rho_0$. Persistence theory measures the so
called lifetime of the excursion set. This is a measure of the absolute
difference in value between a pair of two critical points. By cycling through
values for $\rho_0$ one may measure the lifetime of the topological features.
One can imagine a function of one variable $\rho(x)$ with peaks and throughs as
mountains submerged under a water level set by $\rho_0$. This is illustrated in
figure \ref{fig:persistence}
When the whole function is submerged under water the excursion set is empty. As
the water level $\rho_0$ decrease the first highest peak of the function will
eventually become visible and a new component of the excursion set will start to
grow. Eventually the second peak will appear and a new independent component
will start to grow. These two components are independent of each other because,
as seen from above the surface of the water they are still separated. These two
components will grow untill the through separating the two peaks will become
visible. If the persistence of the pair is lower than a given threshold the two independent components of the excursion set will now merge
together and form a persistence pair. When two components merge one is
destroyed. In this case the lowest maximum is destroyed by the minimum as is
illustrated in figure \ref{fig:persistence}.\\

DisPerSE cancels noise in the dataset by computing the persistance pairs
in the discrete morse complex. Given a persistence pair composed of two
simplicial complexes $q_k=\{\sigma_k,\sigma_{k+1}\}$ on the discrete morse
function $f$, a persistence ratio $r$ is given as
\begin{equation}
    r(q_k)=\frac{f(\sigma_{k+1})}{f(\sigma_k)}.
\end{equation}
This ratio is used for determining the statiscical significance of persistance
pairs. The significance given in "number of sigmas" reads 
\begin{equation}
    S(q_k)=\mathrm{Erf}^{-1}(\frac{P_k(r(q_k))+1}{2}),
\end{equation}
where Erf is the error function and $P_k(r(q_k))$ is the cumulative probability
that a persistence pair with persistence ratio $r\geq r_0$ exists in the
dataset. The code takes a $\sigma$ input for specifying a threshold for cutting
persistence pairs. The argument specifies the persistance threshold in number of
sigmas and any persistance pair with probability lower than the given sigma
threshold will be cancelled.


\begin{figure}\label{fig:persistence}
   \includegraphics[scale=0.6]{persistence.pdf}
   \caption{Må lage bedre figur.}
\end{figure}
\section{REVOLVER-REal-space VOid Locations from surVEy Reconstruction}
For much of the void analysis conducted in this thesis the code
REVOLVER-REal-space VOid Locations from surVEy Reconstruction is utilized. This
code is publicly available on \url{https://github.com/seshnadathur/Revolver}.
In the scope of this project the code provides two important utilites for the
analysis. It provides code for finding voids in a galaxy or halo catalogue and
the ability to reconstruct realspace positions for redshiftspace positions.
\subsection{ZOBOV-ZOnes BOrdering on Voidness}
The void finding method provided by REVOLVER utilized in this project is based
on the ZOBOV-ZOnes BOrdering on Voidness algorithm . The ZOBOV algorithm was
first presented in \cite{Neyrinck_2008}, in which this subsection is based on. In
order to assign a density to the grid from the point particle distribution ZOBOV
takes use of Voronoi-tesselation. Voronoi tesselation divides the space into
individual cells where each cell belongs to a single particle from the point
distribution. Each cell is divided so that a cell around particle $i$ is the
points in space closer to that particle than any other particle. One can imagine
individual bubbles at each particle in the distribution expanding at at an equal
rate. Where these bubbles collide defines the edges of each voronoi cell. The
density of each cell is then $\rho_i=1/V_i$, where index $i$ represents the
density and volume at each cell respectively.\\

After density has been assigned to
the grid through Voronoi-tesselation the particles are divied into zones. The
zones are divided around each density minimum. The center of each zone is a
voronoi cell with a density lower than all of its neighbours. These centers are
found by tracing each particles neighbours until one reaches such a particle with
density lower than all its neighbours. A zone is defined as the set of particles
which density flows downwards towards the minimum. Due to noise present in the
dataset, multiple zones may make up one void. Therefore some of them may need to
be joined together. The way zones are joined is considering, for a 2D density
field and imagining the density function as a scalar function representing a
landscape, individual zones and imagine filling them with water. For each zone
the water level is set to the density at the core of each zone. Gradually
raising the water level other zones will get filled. The process stops when a
zone with a lower core density gets filled, or if the current zone is the one
with the lowest density, when the whole field is filled with water. The water
level in which the water flows into a depper zone is recorded as $\rho(z)$, where $z$ denotes i given zone. This will lead to
a very large void (the zone with lowest density at the center), and many
subvoids. Now one has to define the edges of the voids to determine which zones
should be counted as voids.
\\\indent
One method is to determine the statistical significance of voids. The
probability of wether a zone should be considered as real is determined by the
ratio of the density contrast of a zone with the minimum density of the zone. By
comparing this to a poisson particle distribution one can get the estimate for a
likelihood function $P$ which determines the probability that a void with a
given density contrast could arise from poisson noise. One can then choose to
cut all voids exceeding a given significance level from the dataset.
\subsection{Redshiftspace reconstruction}
Due to all observational data being measured in redshiftspace, in order to study
the realspace positions of observed quantities one has to apply reconstruction. REVOLVER contains a numerical implementation of
the algorithm described in \cite{Nadathur_2018} in which describes the details
of REVOLVER reconstructs realspace positions from redshiftspace. I will give a
short recap of this method here.\\\indent
The algorithm of reconstruction is derived in the framework of Lagrangian
perturbation theory. In this framework the eularian position $\vec{x}(t)$ is
given as
\begin{equation}
    \vec{x}(t)=\vec{q}+\vec{\Psi}(\vec{q},t),
\end{equation}
where $\vec{q}$ is the initial Lagrangian position and $\vec{\Psi}(t)$ is the
displacement field. The eularian position separates itself from the lagrangian
position in the sense that the eularian position is a specific point in space
while the lagrangian position is the position of a parcel where the observer
follows an individual parcel with the velocity field through space and time.
To first order, the displacement field $\Psi$ can be modelled together with the
overdensity as $\nabla_q\Psi^{(1)}{\vec{q},t}=-\delta{\vec{x,t}}$
The displacement field $\vec{\Psi}(\vec{q},t)$ can be modelled together with the
density field as
\begin{equation}
    \nabla\cdot\vec{\Psi}+\beta\nabla\cdot(\vec{\Psi}\cdot\hat{r})\hat{r}=-\frac{\delta_g}{b}
\end{equation}
\cite{recondisplace}, where $\beta$ and the bias $b$ is defined in equations
\ref{eq:beta} and \ref{eq:bias}. By assuming that $\vec{\Psi}$, is irrotational
meaning $\nabla\times\vec{\Psi}=0$, using a Helmholtz decomposition one can
write $\Psi=\nabla\phi$ and
$\beta\nabla\cdot(\vec{\Psi}\cdot\hat{r})\hat{r}\approx\nabla\gamma$.
Uferdig, må se nærmere på utledning