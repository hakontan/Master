\chapter{Background Theory}\label{sec:backgroundtheory}
\section{History of cosmology and how we model the universe}
This section aims to introduce much of the cosmology relevant to the methodology and analysis applied in this thesis.
The following subsections are largely based on \cite{schneider2006extragalactic}, \cite{Dodelson:1282338} and \cite{ryden2017introduction} unless otherwise noted.
\subsection{Distances in the universe and cosmological redshift}\label{sec:sec_distance}
Distances in an expanding universe can prove hard to fathom. In our everyday
lives, we are used to measuring distances in meters and kilometers. The
average distance from the earth to the sun however is approximately $1.49\cdot10^{11}$m.
Although this distance on a cosmic scale is very short, the numbers in our
everyday units of measurement already start to become too big for us to have a
reasonable idea of how long this distance actually is. We have a clear idea of
what a meter and a kilometer is as we have to deal with them every day. To make
things more manageable when measuring these large distances, the average distance
from the earth to the sun has been assigned its own unit of distance measurement known as the
Astronomical unit (AU). This unit is a useful tool for measuring relative
distances in our own solar system, but the star nearest to our solar system, Proxima Centauri, is
located at approximately $268394$AU away from our closest star the Sun.
Therefore it is convenient to introduce another unit of distance measurement known as a light-year. This unit with a value of $9.4607\cdot10^{15}$m is the distance at
which light travels in vacuum during one Julian year equal to $365.25$ days.
This gives us a more manageable account of the distance from the Sun to Proxima
Centauri as $4.244$ light-years \cite{Brown2020GaiaED}. With both the AU and the light-year introduced
we can define the last unit for distance measurement, namely the parsec equal to $3.26$
light-years. One parsec is equal to the distance at which $1$ AU subtends an angle
of one arcsecond. Most
cosmological distances are measured in parsec, abbreviated Pc, with either the
prefix mega or giga as cosmology deals mostly with intergalactic scales much
larger than the average distance between two galaxies.\\

Due to the fact that the universe is expanding, light travelling towards us from
far away objects gets redshifted. Since the expansion of the universe is
isotropic, i.e, the expansion rate of the universe is equal in all directions,
the redshift of this light can be used to measure the distance at which we
observe objects in the universe. The cosmic redshift $z$ is given by the
relation
\begin{equation}
    z = \frac{\lambda_{obs}}{\lambda_{em}} - 1,
\end{equation}
where $\lambda_{obs}$ is the observed wavelength we measure here on earth and
$\lambda_{em}$ is the wavelength emitted by the observed object. By combining this with the doppler formula
$\frac{\lambda_{obs}-\lambda_{em}}{\lambda_{em}} = \frac{v}{c}$, one gets
\begin{equation}
    z = \frac{v}{c},
\end{equation}
where $v$ is the velocity of the observed object relative to the observer. The
velocity of an object due to the expansion of the universe is given by Hubble's law
\begin{equation}
    v = H_0 d_p,
\end{equation}
where $d_p$ is what is known as proper distance and $H_0$ is the Hubble
parameter at present time. The Hubble parameter measures the expansion rate of
the universe as a function of time. The proper distance can be interpreted as the length measured if one
was to use a ruler between two objects $a$ and $b$ at a fixed time $t$ taking
into account the fact that our universe is expanding. Before introducing how proper distance is calculated, it can be
useful to talk about the concept of comoving distance first. If we choose a time
$t=t_0$, which is present time and introduce a coordinate system at this time,
we can choose an arbitrary point to center ourselves at in this coordinate
system. If we apply a ruler in this coordinate system and measure the distance
from our point to another arbitrary point, this is the comoving distance $r$. The
comoving distance is simply the distance measured in a coordinate system at
$t=t_0$. The proper distance on the other hand factors in the expansion of the
universe through the scale factor $a(t)$. The proper distance is given as
\begin{equation}
    d_p = \int_{r_0}^{r_1}a(t)rdr,
\end{equation}
where $a(t)$ is a quantity known as the scale factor. The scale factor parametrizes the relative expansion of the
universe. It is defined in such a way that $a_0$, subscript $0$ meaning
$a(t=t_0)$, is equal to one. This definition of
proper distance also implies that comoving distance is the proper distance at $t=t_0$.
With these properties in place we can now see how redshift, which is a
measurable quantity, can be used to measure the real distance to objects in our universe.\\

Another measurable quantity is flux emitted from an astronomical object.
The flux $F$ is defined as
\begin{equation}
    F = \frac{L}{4\pi R^2},
\end{equation}
where $L$ is the luminosity and $R$ is the distance to the object. Luminosity is a measure
of the absolute electromagnetic power emitted from an astronomical object. The flux, however, measured in $W/m^2$, is simply the luminosity
divided by the area of a shell with a radius at which we observe the object, assuming the radiation is isotropically emitted. Some astronomical phenomena, like a type 1a supernova, have a fixed luminosity
due to the characteristics of the process that creates it. This is called a standard candle. By measuring the observed flux from such a phenomenon, one can then
calculate the luminosity distance as 
\begin{equation}
    d_L = \sqrt{\frac{L}{4\pi F}}.
\end{equation}
\indent
Lastly, we introduce the angular diameter distance. The angular extent of an object in the sky provides us with another measurable quantity related to distances in the universe. The angular diameter distance is, for an object with a known size
$D$ at a large distance $d$, where $d\gg D$, given as 
\begin{equation}
    d_A\equiv \frac{D}{\Delta\theta}.
\end{equation}
Here $\Delta\theta$ is the angle covered by the object measured in radians and $d_P$ is the proper distance. Taking into account the proper distance separating the two endpoints of the diameter of the observed object we have 
\begin{equation}
    D=a(t)r\Delta\theta,
\end{equation}
where $r$ is the distance from the observer to the measured object.
This will give an expression for the angular diameter distance
\begin{equation}
    d_A = \frac{r}{1+z},
\end{equation}
where the relation $a(t)=1/(1+z)$ has been used.
\subsection{Modelling the evolution of the universe and the $\Lambda$CDM model.}
The cosmological principle states that on sufficiently
large scales the universe is isotropic and homogenous. This means the universe
looks the same in every direction for every observer at any point in space for
sufficiently large scales. By sufficiently large scales one means
distances roughly $100$Mpc or larger \cite[p.~12]{ryden2017introduction}. This has proved to be a good
assumption by surveys like the Sloan Digital Sky Survey\cite{sloanhomogenous}. Figure \ref{fig:sdssmap} shows a sky map from a survey released by the SDSS project.\\
\begin{figure}[htbp]
    \includegraphics[scale=0.4]{orangepie.jpg}
    \caption{The SDSS (Sloan Digital Sky survey) map of the universe. Each dot represents a galaxy and the color scale represents the local density. Downloaded from the SDSS website \url{https://www.sdss.org/science/orangepie/}. Image Credit: M. Blanton and SDSS.}
    \label{fig:sdssmap}
\end{figure}
The fact that the universe was homogeneous and isotropic was an essential assumption
for a particular solution of the Einstein field equation. 
This particular equation, first published by Albert Einstein in $1915$ \cite{Einstein1915} concerning his theory of general relativity, is given as
\begin{equation}\label{eq:einstein}
    G_{\mu\nu}=8\pi GT_{\mu\nu}.
\end{equation}
This equation relates $G_{\mu\nu}$, which is the Einstein tensor in which describes the curvature of spacetime, with the energy momentum-tensor
$T_{\mu\nu}$, which describes the energy content of the universe. During the 1920s, by assuming the universe is isotropic, homogenous and expanding, the scientists Alexander Friedmann and Georges Lemaître worked on finding a solution to equation \ref{eq:einstein}. In the 1930s Howard P.Robertson and Arthur Geoffrey Walker further explored the problem and proved that this is the only solution on a spacetime that is homogenous and isotropic.
This solution, named after the four scientists, is known as the Friedman-Lemaître-Robertson-Walker metric
The metric is a tool that allows us to calculate the physical distance in a coordinate system.
For example, for the Cartesian coordinate system, we have the metric 
\begin{equation}
    g_{ij}=
    \begin{bmatrix}
        1 & 0 \\
        0 & 1 
    \end{bmatrix}.
\end{equation}
The definition of the line element $ds^2$ in this case, is given as
\begin{equation}
    ds^2 = \sum_{i,j=0}^1g_{ij}dq^idq^j,
\end{equation}
where $q=(x, y)$ is a vector representing the spatial dimensions and $i$ and $j$ represents the indices.
This gives the familiar line element $ds^2=dx^2+dy^2$.
For a flat universe the Friedman-Lemaître-Robertson-Walker metric for four dimensional space time takes the following form
\begin{equation}
    g_{\mu\nu}=
    \begin{bmatrix}
        1 & 0 & 0 & 0\\
        0 & -a^2 & 0 & 0\\
        0 & 0 & -a^2 & 0\\
        0 & 0 & 0 & -a^2 
    \end{bmatrix}.
\end{equation}
The line element is now given as 
\begin{equation}
    ds^2 = \sum_{\mu,\nu=0}^3g_{\mu\nu}dx^\mu dx^\nu,
\end{equation}
where $dx^0=dt$ and the remaining three indices are for the three spatial coordinates.
This gives us the following line element
\begin{equation}
    ds^2 = dt^2 -a^2(dx^2 + dy^2 + dz^2).
\end{equation}
The energy momentum tensor $T_{\mu\nu}$ for a perfect isotropic fluid takes the
form
\begin{equation}
    T_{\mu\nu}=
    \begin{bmatrix}
        -\rho & 0 & 0 & 0\\
        0 & -a^2P & 0 & 0\\
        0 & 0 & -a^2P & 0\\
        0 & 0 & 0 & -a^2P 
    \end{bmatrix},
\end{equation}
where $P$ is the pressure.
Using this metric together with the Einstein field equations results in the important
equations known as the first and second Friedman equations respectively as
\begin{equation}\label{eq:F1}
    \frac{\dot{a}^2}{a^2} = \frac{8\pi G\rho}{3}
\end{equation}
and
\begin{equation}\label{eq:FII}
    \frac{\ddot{a}^2}{a^2} = -\frac{4\pi G}{3}(\rho + \frac{3p}{c^2}).
\end{equation}
I refer the reader to \cite[ch. 2]{Dodelson:1282338} for an explanation on how to derive the Friedmann equations starting with the Friedman-Lemaître-Robertson-Walker metric.
These differential equations relate the evolution of the scale factor $a$ with the
following properties of the universe, namely density $\rho$, pressure $p$ and gravitational
constant $G$. In these equations $\dot{a}$ and $\ddot{a}$ denotes
derivative and double derivative with respect to time, meaning that $a=a(t)$ is a
function of time. \\

In modern cosmology, the standard model for describing the universe is the $\Lambda$CDM model. Its name is an abbreviation for what 
is considered the main energy contributions governing the expansion of the universe. The cosmological constant $\Lambda$ represents dark energy, CDM is an abbreviation for cold dark matter
and lastly, we have ordinary baryonic matter, which is what we interact with in our
everyday lives. The current estimates suggest that approximately $69\%$ of the
universe consists of dark energy while the remaining $31\%$ is attributed to
matter. The matter contribution is made up of
$27\%$ dark matter and $4\%$ is regular baryonic matter. This leaves only trace contributions from other
energy contributing factors such as photons ($\gamma$) and neutrinos ($\nu$)
\cite{planckparameters}. When modelling the universe these quantities
enter into 
what is called density parameters $\Omega_i$, where $i$ represents a certain type of energy contribution to the universe i.e dark matter or CDM. The density parameter is defined as
\begin{equation}\label{eq:densityparameter}
    \Omega_{i,0} = \frac{\rho_{i,0}}{\rho_{c,0}},
\end{equation}
where $\rho_{i,0}$ is the density of a given energy contribution and $\rho_{c,0}$ is
the critical density of the universe. Subscript zero indicating these quantities
measured at present time. The critical density is the density at which the universe is flat. The critical density at a
given time is given as $\rho_c=\frac{3H(t)^2}{8\pi G}$. For a flat universe, we have
\begin{equation}
    \sum_i \Omega_i = 1.
\end{equation}
By combining both Friedmann equations (equations \ref{eq:F1} and \ref{eq:FII}),
one can derive the following relation for the density $\rho$
\begin{equation}\label{eq:rhoevolution}
    \dot{\rho}=-3\frac{\dot{a}}{a}(\rho+\frac{p}{c^2}).
\end{equation}
By inserting the equation of state $p=w\rho c^2$, where $w$ is a constant,
into equation \ref{eq:rhoevolution} one will get the following differential
equation
\begin{equation}
    \dot{\rho}=-3\frac{\dot{a}}{a}(1+w)\rho.
\end{equation}
This equation can be integrated for an arbitrary $a$ and $\rho$ to present time
giving
\begin{equation}\label{eq:rho_i_evolution}
    \rho_i=\rho_{i,0}a^{-3(1+w_i)},
\end{equation}
where subscript $i$ is again added to denote a specific energy contribution to
the universe. The constant $w_i$ takes different values when considering the
different energy contributions. For CDM and baryons we have $w=0$, for
radiation $w=\frac{1}{3}$ and for the cosmological constant we have $w=-1$. By
taking into consideration equation \ref{eq:F1} and dividing both sides by $H_0^2$,
and recognize the critical density $\rho_{c,0}=\frac{3H_0^2}{8\pi G}$, one can
rewrite the first Friedmann equation as
\begin{equation}
    \frac{H(t)^2}{H_0^2}=\frac{\rho}{\rho_{c,0}}.
\end{equation}
By separating the density into its different components as $\rho=\sum_i\rho_i$
and using equation \ref{eq:rho_i_evolution} for the different components, one can rewrite
the first Friedmann equation using equation \ref{eq:densityparameter} as
\begin{equation}
    H(t)=H_0\sqrt{(\Omega_{CDM,0} + \Omega_{b,0})a^{-3} + \Omega_{r,0}a^{-4} + \Omega_{\Lambda,0}},
\end{equation}
Where $b$ represents baryonic matter and $r$ represents radiation.

\subsection{Einstein-de Sitter model}
While the $\Lambda$CDM model is the leading model for describing the evolution
of our universe, other universe models, being simpler to work with, will also
prove beneficial as approximations for different applications. One of
these models is the Einstein-de Sitter model \cite{1932PNAS...18..213E} proposed
by Albert Einstein and Willem de Sitter in 1932. This model contains only matter
making $\Omega_m=1$. After learning of the expansion
of the universe, from the observations of Edwin Hubble, Einstein removed the
cosmological constant from his equation as this constant was first proposed to keep the
universe static. After the discovery that the expansion of the universe is
accelerating \cite{Goldhaber_2009}\cite{Filippenko_1998} the cosmological
constant was reintroduced. While the Einstein-de Sitter model is long discarded, it is
still useful as it is easy to work with in an analytical framework and provides
good approximations for different applications.\\\indent
In a universe containing only
matter, the first Friedmann equation with $w=0$ takes the form
\begin{equation}
    \frac{\dot{a}^2}{a^2} = \frac{8\pi G}{3}\rho_{m,0}a^{-3}.
\end{equation}
By multiplying with $H_0^2/H_0^2$ and remembering that $\rho_{c,0}=\frac{3H_0^2}{8\pi
G}$, one gets
\begin{equation}
    a\dot{a}^2=H_0^2\frac{\rho_{m,0}}{\rho_{c,0}}.
\end{equation}
Since we consider a flat universe containing only matter we have
$\frac{\rho_{m,0}}{\rho_{c,0}}=\Omega_{m,0}=1$. Taking the square root of both
sides, we get
\begin{equation}
    a^{1/2}\dot{a}=H_0.
\end{equation}
This gives us a differential equation which can be integrated as
\begin{equation}
    \int_{a_0}^a a^{1/2}\mathrm{d}a=H_0\int_{t_0}^t \mathrm{d}t,
\end{equation}
which gives the solution
\begin{equation}
    \frac{2}{3}(a^{3/2}-a_0^{3/2})=H_0(t-t_0).
\end{equation}
By choosing time $t=0$, in which $a=0$, and using that $a_0=1$, we get
\begin{equation}
    \frac{2}{3}=H_0t_0.
\end{equation}
By imposing this boundary condition we find that
\begin{equation}
    a(t)=\Big(\frac{t}{t_0}\Big)^\frac{2}{3}.
\end{equation}

\subsection{Growth of matter perturbations in linear perturbation theory.}\label{sec:linpert}
As I have previously stated, observational data suggest that the universe is isotropic and homogenous. This
however was for sufficiently large scales at around $100$ Mpc or larger. On
smaller scales, the universe is not isotropic and homogenous. A single galaxy,
for example, is denser than the intergalactic medium that separates it from other
galaxies. The anisotropy in temperature of the cosmic microwave background (CMB), as measured by
the Planck satellite and its predecessor CMB experiments, is measured to be
around $\Delta T/T=10^{-5}$. This shows that at around redshift $z=1000$, we had
relatively small fluctuations while we today observe galaxy clusters which has around
$200$ times larger density than the average density of an equal sphere in the
universe \cite[p.~342]{schneider2006extragalactic}. This suggests that the original density
perturbations grows over time. The small structure deviations at early times evolve through what is known as gravitational instability.
Regions with higher density will attract more material through gravity giving a flow of matter towards dense regions making them even denser.
This makes an already irregular distribution of matter become even more irregular with time and will evolve from the small fluctuations at the time of the CMB
to the dense sctructures we observe in our universe today.
To describe this evolution, one defines the relative density
contrast as
\begin{equation}\label{eq:overdensity}
    \delta(\vec{r}, t) \equiv \frac{\rho(\vec{r}, t) - \bar{\rho}(t)}{\bar{\rho}(t)}.
\end{equation}
Here $\bar{\rho}(t)$ denotes the mean matter density of the whole universe at
time $t$, while $\rho(\vec{r}, t)$ denotes the local density at position
$\vec{r}$ at time $t$.\\\indent
The growth of these perturbations can, on scales substantially smaller than the
Hubble radius, which is the radius of the observable universe, be described in
the framework of linear perturbation theory. On these scales, Newtonian gravity
is sufficient to describe the nature of structure growth. With the approximation
that the universe only consists of pressureless matter, which is the case for
cold dark matter, which is the main constituent of the matter in the universe, 
the matter distribution of the universe is modelled as a pressureless fluid. The equations of
motion for such a fluid is governed by the equations
\begin{equation}\label{eq:continuuity}
    \frac{\partial \rho}{\partial t} + \nabla\cdot(\rho \vec{v})=0,
\end{equation}
\begin{equation}\label{eq:eulereq}
    \frac{\partial \vec{v}}{\partial t} + (\vec{v}\cdot\nabla)\vec{v}=-\frac{\nabla P}{\rho}-\nabla \Phi
\end{equation}
and
\begin{equation}\label{eq:poisson}
    \nabla ^2\Phi=4\pi G\rho -\Lambda.
\end{equation}
Here $\vec{v}=\vec{v}(\vec{r},t)$ is the velocity field of the fluid and $\Phi=\Phi(\vec{r},t)$
is the newtonian gravitational potential. $P$ is the pressure of the fluid. Equation \ref{eq:continuuity} is the
continuity equation which tells us that the density changes with the flow of
mass. Equation \ref{eq:eulereq}, which is the Euler equation, describes the
behavior of the fluid under the influence of an external force. The last
equation, equation \ref{eq:poisson}, is the Poisson equation which relates the
gravitational potential to the density field. These equations are solvable in
the limit $\vert\delta\vert \ll 1$, and can then be used to derive an expression
for relatively small density contrasts, which can be used to describe the
evolution of density perturbations on large scales. We can model small perturbations to first order by substituting $\rho =
\rho_0 + \delta \rho$, $\vec{v} =\vec{v_0} + \delta \vec{v}$, $\vec{v} =\vec{v_0}
+ \delta \vec{v}$, $P = P_o + \delta P$ and $\Phi = \Phi_0 +\delta\Phi$. Here
$\delta$ is used to assign small perturbations to the physical quantities and should not be
confused with the previously defined density contrast. The velocity also has to
take into account the expansion of the universe in addition to the peculiar
velocity of the particles giving $\vec{v} = H(t)\vec{r} + \vec{v}_{pec}$. These
equations can then be reduced to 
\begin{equation}
    \frac{\partial^2 \delta}{\partial t^2} + 2H(t) \frac{d \delta}{dt}=4\pi G\bar{\rho}\delta.
\end{equation}
\cite[p.~345]{schneider2006extragalactic}. This equation only contains time derivatives, and none of the coefficients is dependent
on $\vec{x}$. Therefore $\delta(\vec{x}, t)$ can then be expressed as a spatial and time-dependent expression on the form 
\begin{equation}
    \delta(\vec{x}, t) = D(t)g(\vec{x}).
\end{equation}
Here $g(\vec{x})$ is an arbitrary function of the spatial comoving coordinate
$\vec{x}$, and $D(t)$ satisfies
\begin{equation}\label{eq:growthfacD}
    \frac{\partial^2 D}{\partial t^2} + 2H(t) \frac{d D}{dt}=4\pi G\bar{\rho}D.
\end{equation}
This equation has two solutions. One solution is strictly increasing and one is
strictly decreasing. As time evolves the decreasing solution will become negligible
and the increasing solution will dominate. This solution, denoted as $D_+(t)$,
is called the growth factor and determines the amplitude of structure growth as
a function of time. Since $t\propto a$, one can then model the growth factor for
a given cosmological model. We will lastly define the function
\begin{equation}\label{eq:growthfac}
    f(a) \equiv \frac{d\mathrm{ log} D_+}{d\mathrm{log} a},
\end{equation}
to quantify the relationship between the growth of structure and the expansion
of the universe.
\subsection{Spherical collapse models}
Linear perturbation theory has its limitations. In particular, it will not be
able to accurately describe the formation of dark matter halos or galaxy
clusters. The evolution of density contrasts of order higher than unity requires
different techniques to model in a reasonable way. The spherical top-hat model
is one of the most fundamental frameworks to analytically model the non-linear
collapse of overdensities in the universe.
\begin{figure}
    \tdplotsetmaincoords{80}{20}
    \begin{tikzpicture}[tdplot_main_coords]
        \coordinate (O) at (0,0,0);
        \fill[blue!50,opacity=0.5] (1,1,0) -- (10,1,0) -- (10,10,0) -- (1,10,0) -- cycle;
        \node [cylinder, shape border rotate=90, draw,minimum height=3cm,minimum
        width=4cm, fill=blue!50, opacity=0.30] at (5,6,1) (A) {};
        \draw[dashed]
        let \p1 = ($ (A.after bottom) - (A.before bottom) $),
            \n1 = {0.5*veclen(\x1,\y1)-\pgflinewidth},
            \p2 = ($ (A.bottom) - (A.after bottom)!.5!(A.before bottom) $),
            \n2 = {veclen(\x2,\y2)-\pgflinewidth}
         in
         (A.before bottom) arc [start angle=0, end angle=180,
         x radius=\n1, y radius=\n2];

        \draw[->] (O) --++ (1,0,0) node[below] {$x$};
        \draw[->] (O) --++ (0,1,0) node[right] {$y$};
        \draw[->] (O) --++ (0,0,1) node[right] {$\rho$};    
    \end{tikzpicture}
    \caption{A simple illustration of the spherical top hat model.}
    \label{fig:tophat}
\end{figure}
The spherical top-hat model gets its name from how its density profile is
defined. For the spherical top-hat model we have
\begin{equation}
    \rho(\vec{r},t) =
    \begin{cases} 
        \bar{\rho}(1+\delta_i), & \vert\vec{r}-\vec{r}_c\vert\leq R \\
        \bar{\rho}, & \vert\vec{r}-\vec{r}_c\vert \textgreater R,
     \end{cases} 
\end{equation}
where $\vec{r}_c$ is the center of the overdensity and $R$ is the radius.
$\delta_i$ is a factor determining the size of a given overdensity. Figure
\ref{fig:tophat} gives a simple graphical illustration of the model, hence why
it has the name spherical top-hat model. For a spherical shell with the given
density we have the mass
\begin{equation}
    M=\frac{4}{3}\pi R^3\bar{\rho}(1+\delta_i).
\end{equation}
The evolution of the radius $R$ can be modelled through Newtons second law as
\begin{equation}
    \frac{d^2R}{dt^2}=-\frac{GM}{R^2}.
\end{equation}
By multiplying both sides of this equation with $\frac{dR}{dt}$ and integrating
one gets
\begin{equation}\label{eq:kinpot}
    \frac{1}{2}\frac{d^2R}{dt^2}-\frac{GM}{R}=E.
\end{equation}
Here it is worth noting that the left-hand side of \ref{eq:kinpot} is the sum of
the kinetic and potential energy. Therefore the integration constant $E$ can be
interpreted as the total energy. It can be shown that for
$E<0$, this equation has the parametrised solution
\begin{align}
    R&=A(1-\mathrm{cos}\theta)\\
    t&=B(\theta-\mathrm{sin}\theta)\\
    A^3&=GMB^2.
\end{align}
\cite[p.~79]{peebles1980}. By setting an initial radius $R_i$ at time $t_i$ this
solution tells us that a shell expanding with the background evolution of the
universe will eventually slow down and reach a maximum radius at
$\theta=\pi$. This is called turn-around. At this point, the sphere will begin to
collapse. This solution also suggests that at $\theta=2\pi$ the sphere would
collapse to a point. This is not the case as something known as virialization
occurs before this happens.\\\indent
The virial theorem describes the relationship between the potential and kinetic
energy in dynamical equilibrium in a set of collisionless particles under the influence of a gravitational
potential. Individual particles in the gas will reach $R=0$ and eventually
shoot outwards again. The whole system will stabilize when it has virialized. The virial theorem states that
\begin{equation}\label{eq:virial}
    \vert U\vert=2K,
\end{equation}
where $U$ is the potential energy and $K$ is the kinetic energy.
At the radius of turn around, the kinetic energy is zero. As the cloud collapses
the kinetic energy will increase and eventually it will virialize when the
conditions of equation \ref{eq:virial} is fulfilled.
\subsection{Two point correlation functions and the matter power spectrum.} \label{sec:corrtheory}
Matter in the universe is not randomly distributed in a uniform fashion. Starting with the initial perturbations, matter clumped together and formed galaxies and clusters as a result of gravitational instability. This means that for a given random point in space, the probability of finding
a galaxy in the vicinity of that point is not independent of the surrounding galaxy distribution. The statistical distribution
of galaxies is described by the two-point correlation function. The two-point correlation function for galaxies $\xi_{g}(r)$ describes the probability of two galaxies are separated by a distance $r$. The correlation function is related to the density contrast as
\begin{equation}
    \xi(\vert\vec{r_1}-\vec{r_2}\vert)=\langle\delta(\vec{r_1})\delta(\vec{r_2})\rangle.
\end{equation}

The matter power spectrum $P(k)$ provides a statistical description of the
distribution of matter in the universe, but in Fourier space. Figure
\ref{fig:matterpowerspec} shows the matter power spectrum inferred from a multitude
of cosmological surveys. The matter power spectrum describes the amplitude of 
the density contrast on different length scales $L=\frac{2\pi}{k}$, where $k$ is
the Fourier wave number. The matter power spectrum is related to the two-point
correlation function as a Fourier transform given by
\begin{equation}
    P(k)=2\pi\int_0^\infty x^2\frac{\mathrm{sin}(kx)}{kx}\xi(x)dx.
\end{equation}
\begin{figure}[htbp]
    \includegraphics[scale=0.7]{matterpowerspec.pdf}
    \caption{The matter power spectrum inferred from different cosmological surveys. Image credit: "ESA and the Planck Collaboration" \cite{2020}.}
    \label{fig:matterpowerspec}
\end{figure}
Both of these quantities are statistical properties and are of high importance
when comparing models to observations or simulations. The initial conditions for 
the origin of structure in our universe are coupled to the period of Inflation. Inflation is a rapid expansion
of space that occurred in the infancy of the universe after The Big Bang.
Inflationary theory does not predict the exact location of the distributed matter.
Instead, it predicts how it should be statistically distributed.
A simulation or calculation of structure growth may
not look identical to anything anywhere in the universe (if it is not infinitely
large), but if the statistical properties are the same, then the models are
correct. It is therefore important to
note that two universes are considered identical if they inhabit the same
statistical properties. This makes the power spectrum and two-point correlation function
important vital tools when studying the universe.
\\\\\indent
An important cosmological parameter is $\sigma_8$ which serves as a normalization for the matter power spectrum. The parameter $\sigma_8$ is motivated by the observation that when averaging over spheres on scales of radius $R=8$Mpc/h it is found that
\begin{equation}
    \sigma_{8,g}^2\equiv\Big\langle\Big(\frac{n-\bar{n}}{\bar{n}}\Big)^2\Big\rangle_{8\mathrm{Mpc/h}}\approx1,
\end{equation}
where $n$ is the local number density of galaxies and $\bar{n}$ is the average number density.
In a similar fashion, it is defined through the dark matter density contrast as
\begin{equation}
    \sigma_8^2=\Big\langle\vert\delta_{8\mathrm{Mpc/h}}\vert\Big)^2\Big\rangle,
\end{equation}
where the density contrast $\delta$ is averaged over spheres of $R=8$Mpc/h.
These two parameters can be related through the linear bias factor as
\begin{equation}\label{eq:sigma_8_relation}
    \sigma_8=\frac{\sigma_{8,g}}{b}.
\end{equation}
Since the overdensity of galaxies does not
necessarily follow the overdensity of dark matter, in which is used for the derivation of linear perturbation
theory, it is scaled by a bias factor
\begin{equation}\label{eq:bias}
    b=\frac{\delta_g}{\delta_{dm}},
\end{equation}
where subscript $g$ is for galaxies and $dm$ is for dark matter.
This way, one can normalize the dark matter power spectrum by measuring the $\sigma_{8,g}$ parameter and using the relation in equation \ref{eq:sigma_8_relation} to use scales on $8$Mpc/h as a reference for normalizing the power spectrum when the shape is known.
\subsection{The cosmic web}
On large scales the universe is isotropic and homogenous. However this does not
mean that every galaxy or cosmological particle is distributed randomly. As
previously discussed, matter tends to flow towards overdensities. Over time many
of these overdensities will collapse into large dark matter halos. Dark matter
halos contain subsctructures and sub halos where baryonic matter can form
galaxies. The massive halos are bound together by gravity in what resembles a
web-like structure known as the cosmic web. The cosmic web is made up of large
filaments of baryonic and dark matter, and in between there are large voids with
very low density. Figure \ref{fig:millenium} shows the Millenium simulation with
$10^{10}$ dark matter particles at different redshifts $z=0$ and $z=18.3$ corresponding to
$t=0.21$Gyr and $t=13.6$Gyr after the big bang respectively.
\begin{figure}[htbp]
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/milleniumz0.jpg}}\hspace{1em}%
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/milleniumzoomz0.jpg}}
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/milleniumz18.jpg}}\hspace{1em}%
    \subfigure[]{\includegraphics[width=0.5\textwidth]{figures/milleniumzoomz18.jpg}}
    \caption{Figure showing snapshots of the Millennium simulation with $10^{10}$ particles
    for two different redshifts. Figures a) and b) is taken at $z=0$ while
    figures c) and d) is taken at $z=18.3$. Image credit: Downloaded from the
    website of the Millennium Simulation\cite{Millennium} (\url{https://wwwmpa.mpa-garching.mpg.de/galform/virgo/millennium/}).}
    \label{fig:millenium}
\end{figure}
Here one can clearly see that as time evolves the matter coalesces from a
relatively even distribution of matter into a web-like structure with filaments
and voids. The same pattern is also visible in the SDSS sky survey shown in
figure \ref{fig:sdssmap}.\\\indent
An important phenomena for the matter to coalesce into this web-like structure is
baryonic acoustic oscillations (BAO). Baryonic acoustic oscillations originate
from the dense hot plasma that the universe consisted of after the big bang.
During this time photons and baryonic matter was coupled together. While dark
matter is collisionless, the baryon-photon plasma on the other hand is not and
is subject to pressure forces. Gravity will cause matter to flow towards
overdensities. The collisionless dark matter will clump together and form
potential wells, while the baryonic matter is subject to outward pressure as it clumps
together. While the photons and baryons are coupled together they move with the
sound speed, which in this plasma is roughly half the speed of light. The
outwards pressure will cause ripples of baryonic matter flowing outwards from the
dark matter overdensities. At approximately $z\approx 1000$ baryons and photons
decoupled. In cosmological history this period is called the epoch of
recombination. Due to the expansion of the universe, the experienced interaction rate between
photons and baryonic matter decreased drastically. This caused the
rate at which photons scattered off of baryonic matter to drastically decrease. This allowed for the formation of neutral hydrogen.
As baryonic matter was no longer bound to photons this also caused the sound
speed to drop eventually causing the ripples to stop expanding. This lead to
shells of baryonic matter eventually attracting
portions of dark matter away from the overdensities in the center of the
potential wells formed before recombination. These ripples are not directly
observed but can be inferred from measuring the correlation function for
galaxies. The correlation function contains a bump at the characteristic length
scale $r_s$. This shows that the baryonic acoustic oscillations are influential on the distribution of
matter in the universe today.

\subsection{Real- vs Redshift space}
As mentioned earlier in section \ref{sec:sec_distance}, due to the fact that the
universe is expanding, galaxies we observe in the universe on cosmological scales
move away from us and appear redshifted. In addition, galaxies also have peculiar
velocities, which is their velocity when subtracting the Hubble flow. This results in an effect, that when observing galaxies
as a function of redshift, their spatial distribution appears squashed. As most
cosmological surveys of galaxies are done by measuring their redshift,
understanding how this affects their spatial distribution is crucial.
We will now denote real space coordinates by subscript $r$ and redshift space
coordinates by subscript $s$. By taking into account the peculiar velocity of
galaxies and the Hubble flow, we can express their redshift as a sum of two
terms
\begin{equation}\label{eq:losrs}
    cz=H_0d_p+v_{pec}.
\end{equation}
Here the first term represents the Hubble flow and the second term is the
peculiar velocity along the line of sight. For most galaxies, only the redshift
can be measured and not the proper distance. Therefore, by dividing equation
\ref{eq:losrs} by $H_0$, it is useful to define
the redshift distance $s$ as
\begin{equation}
    s = \frac{cz}{H_0}=d_p+\frac{v_{pec}}{H_0}.
\end{equation}
\\

The peculiar velocity of galaxies can also have a profound effect on how their
spatial distributions appear in redshift space. Matter will cluster and move
towards overdensities. This means that the velocity component of galaxies in a
cluster will essentially point towards the center of the galaxy cluster. This
will give an effect that when viewing a galaxy cluster, the galaxies closest to
us in real space will have their peculiar velocity move away from the observer.
From equation \ref{eq:losrs} one can see that this will give an additional
contribution in addition to the Hubble flow and the measured redshift will be
higher. Galaxies on the opposite side of the cluster as seen by the observer
will have their peculiar velocity point towards the observer. This will cause
the peculiar velocity to contribute with a blueshift, and likewise, the measured
redshift will appear smaller than if one could only measure the redshift due to
the Hubble flow. This effect will cause galaxy cluster to appear squashed in
redshift space. This effect will also cause the overdensity to appear much larger
in the center of a collapsed object than in real space. This is illustrated in
figure \ref{fig:rsddistortion}.\\

\begin{figure}[htbp]
    \begin{tikzpicture}
        \draw (5,0) circle (3cm) node[above] {Realspace};
        \draw (13,0) ellipse (3cm and 1cm) node[above] {Redshiftspace};
        \draw[line width=0.5mm][->] (5,3cm) -- (5,1.5cm);
        \draw[line width=0.5mm][->] (5,-3cm) -- (5,-1.5cm);
        \draw[line width=0.5mm][->] (5+1.8cm,0) -- (5+3.5cm,0);
        \draw[line width=0.5mm][->] (5+7.8cm,0) -- (5+5.8cm,0);
    \end{tikzpicture}
    \caption{Figure illustrating a collapsing halo in realspace and redshiftspace. The vector pointers represent the direction of the peculiar velocity. In this illustration both circles are meant to contain the same number density of galaxies thereby giving the apparent overdensity appear much higher in redshiftspace than in realspace. The observer sitting at the bottom of the page with the line of sight direction being parallell to the plane spanned by the circle and the ellipse.}
    \label{fig:rsddistortion}
\end{figure}
The effect of redshift space distortions causes the correlation function and thereby
matter power spectrum to be slightly altered when observed in redshift space. In
Fourier space, the overdensity receives a correction term as\cite[p.~279]{Dodelson:1282338}
\begin{equation}
    \delta_s(\vec{k})=(1+\beta\mu^2)\delta(\vec{k}).
\end{equation}
Here $\mu=\vec{z}\cdot\vec{k}$ is the angle
between the line of sight and the Fourier wave vector. The factor $\beta$ can be
seen as the growth factor $f$ introduced in equation \ref{eq:growthfac} from
section \ref{sec:linpert}. However, it is scaled by the linear bias factor given in equation \ref{eq:bias} as
\begin{equation}\label{eq:beta}
    \beta=f/b.
\end{equation} 
\section{Cosmological N-body simulations}
Analytical approaches, such as linear perturbation theory, has their limits.
Especially when taking into consideration small scale interactions between
galaxies. Using linear perturbation theory, gravitational interactions on small scales can not be described in
enough detail to accurately compare observational data to theoretical
predictions. Therefore, numerical simulations of structure formation is an
important tool when studying the properties of the universe. Numerical
simulations have been crucial for establishing $\Lambda CDM$ as the standard
model for cosmology. Only through simulations has it become possible to separate
the predictions of different models when comparing models to observations
\cite[p.~361]{schneider2006extragalactic}. The first cosmological N-body
simulation was conducted by \cite{PeeblesPJE1970SotC}, where the equations of
motion for $300$ particles were solved to study the formation of galaxy clusters. Due to the significant increase in
computer power over the last few decades, one has been able to perform
increasingly more detailed numerical simulations where one can afford to
increase both the spatial and temporal dimension in the simulations.\\

When simulating large scale structure formation in the universe, it is mostly
sufficient to take into account dark matter, as this is the primary contributor
to the mass in the universe. The size of dark matter particles will be a lot smaller than the simulation volume. Therefore
all the dark matter particles in the simulation are represented by point
particles with mass $M$, where each point particle is a body of mass
representing multiple dark matter particles. One also has to restrict the
simulation volume of the simulation. The size of the universe is too large to
simulate in detail. It may also be infinite. Therefore one has to select a
simulation volume that is representative for the effects one wants to model. For
simulations where one wants to examine large scale structures, the simulation
volume is usually a box with size $L>200$Mpc, as large scale structures are hardly present on
scales smaller than this \cite[p.~362]{schneider2006extragalactic}. Due to
restricting the simulation to a small boxed slice of the universe, problems with
the boundaries of the simulation volume will rise. If not treated, particles at
the boundary of the simulation volume will not be affected gravitationally by as many
neighbouring particles as particles in the middle of the simulations volume. To
account for the fact the universe should be isotropic and homogenous on large
scales one has to apply periodic boundary conditions. This means that the cube
is extended periodically. A particle leaving the volume in one end will reappear
in the other end. Particles will also interact gravitationally in the same
manner meaning that particles on opposing sides of the simulation volume will
affect each other as if the simulation volume was extended by adding an
identical volume side by side.

\subsection{Pairwise summation}
The simplest and most intuitive way of performing N-body simulations is by
modelling gravity as a force and applying Newton's second law to calculate the
acceleration vector of the particles. For every particle, one calculates the
force acting on it from all other particles by summing over the contribution
from all other particles. The computational cost of this approach scales as
$N^2$, where $N$ is the number of particles in the simulation. This makes this
approach not feasible when including $N\gtrsim10^6$ particles. However
interesting problems can be studied by not exceeding this number of particles. This problem can be formulated mathematically as
\begin{equation}\label{eq:newtongravacc}
    \frac{d^2r_i}{dt^2}=-G\sum_{i=1}^{N}\sum_{j\neq i}^N\frac{m_j}{\vert\vec{r}_j-\vec{r}_i\vert^3}(\vec{r}_j-\vec{r}_i),
\end{equation}
where the acceleration of the $i$th particle with position
$\vec{r}_i$ is calculated by summing over all particles $j\neq i$ with position $\vec{r}_j$.
When studying equation \ref{eq:newtongravacc}, one can see that if two particles
come very close to each other, numerical instability will occur in the form of a division by
zero, which in turn will result in an unreasonably high force calculation. To counteract this problem
force softening is introduced. A way to do this is to modify the denominator of
equation $\ref{eq:newtongravacc}$ by including a small factor $\epsilon$
replacing $\Delta r_{i,j}=\vert\vec{r_j}-\vec{r_i}\vert$ with $\sqrt{\Delta
r_{i,j}^2+\epsilon^2}$\cite[ch.2]{Fazio2309855}. \\\indent
We will now use the notation where the acceleration is
given as $a(t)=d^2r(t)/dt^2$ and the velocity $v(t)=dr(t)/dt$.
From equation \ref{eq:newtongravacc} we now have an expression for the acceleration of each object.
This gives us the coupled system ODEs necessary to solve for the position as
\begin{equation}
    v(t)=\frac{dr(t)}{dt} \quad\mathrm{and}\quad a(t)=\frac{dv(t)}{dt}.
\end{equation}
We now discretize the time variable such that $t_n$ is the time at a specific
time point, where $t_n=t_0+n\Delta t$, with $\Delta t$ representing an incremental
time step. By a Taylor expansion around $t_n$, one can approximate the time
solution at time step $t=t_n+\Delta t$ giving
\begin{equation}
    r(t_n+\Delta t) = r(t_n) + \frac{dr(t_n)}{dt}\Delta t + \frac{1}{2}\frac{d^2r(t_n)}{dt^2}\Delta t^2 +\ldots
\end{equation}
and
\begin{equation}
    v(t_n+\Delta t) = v(t_n) + \frac{dv(t_n)}{dt}\Delta t + \frac{1}{2}\frac{d^2v(t_n)}{dt^2}\Delta t^2 +\ldots.
\end{equation}
By discretizing the spatial variables in the same way as the time variable, one can
write $r(t_n)=r_n$, $v(t_n)=v_n$ and $a(t_n)=a_n$ respectively for every particle.
Similarly we have $r_{n+1}=r(t_n + \Delta t)$ etc.
By truncating the Taylor expansion at the second order term one can then write
an approximate solution to our problem as
\begin{center}
\begin{itemize}
    \item $a_n=-\sum_{i=1}^{N}\sum_{j\neq i}^N\frac{Gm_j}{(\Delta
    r_{i,j}^2+\epsilon^2)^{2/3}}(\vec{r_j}-\vec{r_i})$
    \item $v_{n+1} = v_n + a_n\Delta t$
    \item $r_{n+1} = r_n + v_n\Delta t.$
\end{itemize}
\end{center}
This is done for every particle $i$ for every time step $t+\Delta t$. This is one
of the simplest ways to numerically solve this system of coupled ODE's and is called the forward
Euler method. This approach however is never used in practice as the error of
the solution, denoted $\epsilon$, as can be seen from the Taylor expansion, is of order
$\epsilon\sim\mathcal{O}(\Delta t^2)$. Another significant disadvantage with this method is
that when solving for physical systems, it will not conserve energy. A numerical
integration scheme well suited for gravitational problems where energy is
conserved is the Velocity Verlet algorithm. This integration scheme is what is
known as symplectic, meaning that it conserves the energy of the system
\cite[p.~31]{holmes2007introduction}. The Velocity Verlet scheme updates its
velocity and position as
\begin{center}
    \begin{itemize}
        \item $r_{n+1} = r_n + v_n\Delta t+\frac{a_n}{2}\Delta t$
        \item $v_{n+1} = v_n + \frac{\Delta t}{2}(a_{n+1}+a_n).$
    \end{itemize}
\end{center}
This method, and its variations, that preserve energy over long time periods
makes it a preferred method of choice for astrophysical gravitational problems.
\subsection{Particle mesh}
Another way of simulating a box with a large volume of particles is to apply
what is called a Particle Mesh code \cite{HockneyRW1981Csup} \cite{Fazio2309855}. This method is more computationally
effective as the number of calculations scales proportionally to the number of particles $N$. However, most of
the information is stored on a $3D$ grid covering the whole simulation volume.
This will require more memory as the resolution of the grid increases.\\\indent
The Poisson equation for gravity reads
\begin{equation}\label{eq:poissongrav}
    \nabla^2\Phi=4\pi G\rho.
\end{equation}
This equation can be solved for the gravitational potential $\Phi$ and
the force field updated as $\vec{F}=-\nabla\Phi$. This method is reliant on assigning a
density field to the simulation volume of individual particles as a
representation for the density $\rho$ in equation \ref{eq:poissongrav}. One popular method 
for this is what is known as Cloud In Cell (CIC). One wants to calculate the density $S$ at a given
distance, given by the coordinate point $(x, y, z)$, from the particle. The density at a given point is given as
$S(x)S(y)S(z)$, where the CIC scheme for assigning a value to $S$
reads
\begin{equation}
    S(x)=\frac{1}{\Delta x}
    \begin{cases}
        1, &\quad\text{if }\vert x \vert < \Delta x/2\\
        0, &\quad\text{otherwise},
      \end{cases}
\end{equation}
where $\Delta x$ is the cell size. The density assigned to the grid is defined
by a product of three weight functions $W(\vec{r}_p -\vec{r}_{ijk}) =
W(x_p-x_i)W(y_p-y_j)W(z_p-z_k)$. Where subscript $p$ denotes particle position
and subscript $i,j,k$ denotes a given grid index. A given $W$ is expressed as
\begin{equation}
    W(x)=\int_{x_i-\Delta x/2}^{x_i+\Delta x/2} S(x_p - x^\prime)dx^\prime.
\end{equation}
We then have that the density assigned to a given grid point for a grid
consisting of $N$ particles is given by the relation
\begin{equation}
    \rho_{ijk}=\sum_{p=1}^Nm_pW(\vec{r}_p-\vec{r}_{ijk}),
\end{equation}
where $m_p$ is the mass of particle $p$.
With the density assigned, one can then solve the Poisson equation for gravity given
by equation \ref{eq:poissongrav}. This is done by using a Fast Fourier Transform
(FFT) on the whole grid giving
\begin{align}
    \nabla^2\Phi&=4\pi G\rho\\
    -k^2 \hat{\Phi}&=4\pi G \hat{\rho},
\end{align}
where $\hat{\Phi}$ and $\hat{\rho}$ denotes the Fourier transform of the given
quantities. Dividing by $k^2$ and taking the inverse transform one solves the
equation for the gravitational potential as
\begin{equation}
    \Phi = \mathrm{IFFT}[-\frac{4\pi G \hat{\rho}}{k^2}].
\end{equation}
With the gravitational potential, one can calculate the force and interpolate
forces to their respective particle positions in the grid. The particle mesh
method follows the same algorithm as the pairwise summation method but differs
in the force calculation. Summarized the force calculation of the particle mesh
method consists of three steps:
\begin{itemize}
    \item Assign density to grid. \\
    \item Solve the Poisson equation given by equation \ref{eq:poissongrav}.\\
    \item Compute the force and apply to particles in the grid.
\end{itemize}
While the computation time is significantly decreased the
particle mesh method lacks in terms of resolution. Increasing the resolution for a
particle mesh code means increasing the number of bins in each direction. For a
three dimensional grid, this will quickly put constraints on the computer memory.
\subsection{Tree codes and Barnes-Hut simulations}\label{sec:barneshut}
\begin{figure}
    \includegraphics[scale=0.6]{figures/barneshut.png}
    \caption{Illustration of a Barnes-Hut tree for 100 particles in 2D space. Image downloaded from \url{https://commons.wikimedia.org/wiki/File:2D_Quad-Tree_partitioning_of_100_bodies.png}.
    The file is licenced under the following license \url{https://creativecommons.org/licenses/by-sa/3.0/deed.en}.}
    \label{fig:barneshut}
\end{figure}
The particle mesh and the pairwise summation methods are the simplest forms of N-body simulation methods.
In modern codes however these methods are rarely used in exclusivity. Instead, in many modern codes, the methods are based on the approach proposed
by Barnes and Hut in 1986\cite{Barneshut}. This method divides the simulation volume into what is called a tree. An example of such a tree in 2D space is 
illustrated in figure \ref{fig:barneshut}. The volume in 2D is divided into four cells or eight cells in 3D. If the number of particles in each cell exceeds a certain
threshold specified by the user, the given cell is then divided into four or eight more cells depending on whether the simulation is in 2D or 3D. This dividing of cells
into smaller cells continues until no cells contain a particle number higher than the set threshold. After creating the tree, the forces have to be calculated. When calculating
the force on a given particle, the distances of particles in other cells becomes crucial. The basic idea is that if a certain number of particles are far away from a given particle,
one can calculate the center of mass for a larger cell containing more particles. This greatly reduces the number of calculations as the force from far away particles can be approximated
as a single force from one large body instead. The advantage with this approach is that, instead of the calculations scaling as $\mathcal{O}(N^2)$ as for the regular pairwise summation,
it scales as $\mathcal{O}(N\mathrm{log}N)$ instead. This approach briefly described here is fundamental for the approach for many high-level cosmological N-body simulation codes.

\subsection{Initial conditions}
When setting the initial conditions for cosmological simulations, one starts at
a high redshift. The initial distribution of the particles resembles a gaussian
random field with the theoretical matter power spectrum $P(k,z)$ for the given
cosmological model. Choice of a time step is also important. One has to find a
compromise between long computation time and accuracy between closely
interacting particles. 

\section{Statistics}
For the analysis of the data, statistical methods are utilized. This section
gives a brief overview of the fundamental methods used for basic statistical analysis.
This section is largely based on \cite{Probphysics} and \cite{Dataanalysiscosmology}.
\subsection{Bayes theorem}
When studying and comparing our observations and simulations with theory one has
to quantify the probability of the models parameter space given the data used.
I.e, what parameters will make our model compare best with our observations or
simulations. Consider the probability $P(A, B)$, which is the probability for two events $A$ and $B$ occurring.
If the two events $A$ and $B$ are independent of each other, the probability is simply given as $P(A, B) = P(A)P(B)$.
However, this is not the case if the events $A$ and $B$ are dependent on each other. Take for instance the probability
of drawing two knights in a row from a deck of cards. The first draw alters the number of cards in the deck and will therefore affect
the probability of the second draw. We write this as $P(A,B) = P(A)P(B\vert A)$. Here $P(B\vert A)$ denotes the probability of $B$ given the fact that 
event $A$ has occurred. We can likewise write $P(A,B) = P(B)P(A\vert B)$. Now let's consider the event $A$ being the parameters of our model $\theta$, and
event $B$ being our data represented by $d$. We can then write $P(\theta,d) = P(\theta)P(d\vert \theta)=P(d)P(\theta\vert d)$. Rearranging terms we will get
what is known as Bayes theorem\cite{Probphysics}
\begin{equation}\label{eq:bayes}
    P(\theta\vert d) = \frac{P(d\vert \theta)P(\theta)}{P(d)}.
\end{equation}
$P(\theta\vert d)$ is what is known as the posterior and quantifies an
important question one would ask when studying cosmology or physics in general. What is the probability
of the parameters of the model being correct given the data one is studying? Likewise $P(d\vert\theta)$ is the
probability of the data being true given the parameters one has tested. This is what is known as
the likelihood and is usually denoted as $\mathcal{L}(\theta)$. $P(\theta)$ is what is
known as the prior. This is used to quantify what we already know about our
parameter space. $P(d)$ is what is known as the evidence and acts as a
normalization factor. \\

In most practical examples our model contains multiple parameters we want to fit. Let's say we have a model
dependent on the parameters $\theta_0$ and $\theta_1$. We will then get the joint probability distribution $P(\theta_0, \theta_1)$.
If we want to know the probability of $\theta_0$ independently of $\theta_1$, we can get what is known as the marginal distribution given by
\begin{equation}
    P(\theta_0)=\int_{-\infty}^{\infty}P(\theta_0, \theta_1)d\theta_1
\end{equation}

\subsection{Parameter estimation}
When comparing models with data, one has to find an expression to quantify the
agreement between the model and the data. An important statistical tool is what
is called the $\chi^2$ test. Given two functions $y^{\mathrm{model}}_i$ and $y^{\mathrm{data}}_i$,
representing the model and data respectively, where $i$ is a given data point,
the $\chi^2$ becomes
\begin{equation}
    \chi^2=\sum_{ij}(y^{\mathrm{model}}_i-y^{\mathrm{data}}_i)C_{ij}^{-1}(y^{\mathrm{model}}_j-y^{\mathrm{data}}_j).
\end{equation}
In this expression, $C_{ij}$ is the covariance matrix if the data are correlated. If the data
is uncorrelated, the $\chi^2$ takes a simpler form
\begin{equation}
    \chi^2=\sum_{i}\frac{(y^{\mathrm{model}}_i-y^{\mathrm{data}}_i)^2}{\sigma_i^2},
\end{equation}
where $\sigma_i$ is the error estimate for the given data point $i$. This is a
useful tool for testing the scatter of our data. The likelihood function
$\mathcal{L}(\theta)=P(d\vert \theta)$ describes the probability of our dataset
$d$ being true given our parameters. By holding our dataset constant one can
maximize the likelihood function. In other words, one can try to find the most
probable value for $\theta$. If one assumes the data is given by a gaussian
distribution, the likelihood function is defined by a multivariate gaussian
\begin{equation}
    \mathcal{L}(\vec{\theta}) = \frac{1}{\sqrt{(2\pi)^n\vert C \vert}}\mathrm{exp}\big[{-\frac{1}{2}}\sum_{ij}(y^{\mathrm{model}}_i-y^{\mathrm{data}}_i)C_{ij}^{-1}(y^{\mathrm{model}}_j-y^{\mathrm{data}}_j)\big],
\end{equation}
where $n$ is the number of parameters. From this, we can see that the likelihood and the $\chi^2$ is related as
$\mathcal{L}\propto \mathrm{exp}[-\frac{1}{2}\chi^2]$. From this, it is also evident that
maximizing the likelihood is the same as minimizing $\chi^2$. Using Bayes theorem given by equation
\ref{eq:bayes} one can see that we now have an expression for the likelihood.
The evidence $p(d)$, which acts as a normalization factor is not important as we
want the relative probabilities of our parameters and therefore this term can be
ignored \cite{heavens2010statistical}. If one chooses a flat prior, one where
$p(\theta)$ is constant, one simply gets by addressing Bayes theoerem that
\begin{equation}
    P(\theta\vert d) \propto \mathcal{L}(\theta).
\end{equation}
\subsection{Sampling and Metropolis-Hastings MCMC}
When dealing with distributions $P(\vec{\theta},\vec{d})$, where there is a
large amount of parameters $\vec{\theta}=(\theta_0, \theta_1,\dots, \theta_n)$,
a simple gridding algorithm can prove to be both time and memory consuming. If
we also take into account the fact that one may deal with gaussian
distributions, there are large areas of parameter space where the probability of
particular configurations of parameters being true are slim to none.
Therefore, by gridding, one spends a significant amount of time exploring a relatively
uninteresting part of parameter space. Therefore, it is useful to introduce the
concept of sampling and the Metropolis-Hastings Markov-Chain Monte Carlo
method.\\

The Monte Carlo principle states that if you take
independent and identically distributed samples $\theta$ from an arbitrary
function $f(\theta)$, as the number of samples increases, the distribution of
samples will converge towards the function $f(\theta)$. This is expressed
mathematically as
\begin{align}
    f_N(\theta) &= \frac{1}{N}\sum_{i=1}^N\delta(\theta)\\
    &\rightarrow \lim_{n\to\infty}f_N(\theta)=f(\theta)
\end{align}
There is however a problem in that the function we want to calculate
$P(\theta\vert d)$ is a distribution that is unknown to us. We
know how to calculate it, but we cannot draw from it. Therefore one introduces a
proposal distribution $Q(\theta)$. This is a known function that one can easily
evaluate and pick samples from. The proposal distribution can be any function,
but it is usually a Gaussian or any other function enclosing the parameter space
of the target function. The target function is the function one wants to
represent, but it may be computationally ineffective to use a brute force grid
approach if the model contains a multitude of parameters. In our case, the
target distribution is the likelihood or the posterior distribution.\\\indent
One of the
most popular Markov chain Monte Carlo algorithms is the Metropolis-Hastings
algorithm. The basic idea behind the Metropolis-Hastings algorithm is to draw a
point $\theta^*$ from the proposal distribution $Q(\theta)$ based on the
previous draw, and compare the draw with an acceptance criterion. The probability for the acceptance criterion is usually drawn
from a uniform distribution. The acceptance criterion in
Metropolis-Hastings is given by
\begin{equation}\label{eq:acceptance}
    \alpha = \mathrm{min}\big[1, \frac{P(\theta^*)Q(\theta^*\vert \theta)}{P(\theta)Q(\theta\vert\theta^*)}\big],
\end{equation}
where $Q(\theta^*\vert\theta)$ represents the proposal distribution for a new
pick of parameters $\theta^*$ based on previous parameters. $P(\theta)$ is
the target density distribution. After
drawing an initial value $\theta_0$ from the proposal distribution $Q$, the
Metropolis-Hastings algorithm is a repeatable sequence of the following
\begin{itemize}
    \item Sample $\theta^*$ from $Q(\theta^*\vert\theta)$ \\
    \item Sample a number $u$ from the normal distribution \\
    \item if $u < \alpha$, where  $\alpha$ is given by equation \ref{eq:acceptance}: \\
          \indent accept move and sample $\theta^*$\\
    \item else:\\
          \indent reject the move and sample the previous point $\theta$ instead.\\
    \item Repeat until you have a sufficient amount of samples to accurately represent the
    target distribution.
\end{itemize}
The idea with the acceptance criterion is that the algorithm will eventually
explore all of the parameter space where the target distribution has its highest values. If the algorithm just
accepted the next step without comparing it to a random pick from the normal
distribution, the algorithm would just converge to the closest peak. By
comparing the acceptance criterion to a pick from the normal distribution, one
would also in some cases accept picks with a less probable state. This makes the
algorithm map the whole distribution, but it will quickly drift away from
spending too much time exploring parameter space where the target distribution is negligible. 
