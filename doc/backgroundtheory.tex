\chapter{Background Theory}
\section{History of cosmology and how we model the universe}
Skrive noe generelle ting om kosmologi og astronomi historie her.
\subsection{Distances in the universe and cosmological redshift}
Distances in an expanding universe can prove hard to fathom. In our everyday
lives we are used to measuring distances in meters and kilometers. The
average distance from the earth to the sun however is approximately $1.49\cdot10^{11}$m.
Allthough this distance on a cosmic scale is very short, the numbers in our
everyday units of measurement already start to become too big for us to have a
reasonable idea of how long this distance actually is. We have a clear idea of
what a meter and a kilometer is as we have to deal with them everyday. To make
things more manageable when measuring these large distances the average distance
from the earth to the sun has become its own distance unit known as the
Astronomical unit (AU). This unit is a great tool for measuring relative
distances in our own solar system, but our nearest star, Proxima Centauri, is
located at approximately $268394$AU away from our closest star the Sun.
Therefore it is convenient to introduce another unit of distance measurement known as light year. This unit with a value of $9.4607\cdot10^{15}$m is the distance at
which light travels in vacuum during one Julian year equal to $365.25$ days.
This gives us a more manageable account of the distance from the Sun to Proxima
Centauri as $4.244$ light years. With both the AU and the light year introduced
we can define the last unit of measuring distance, namely the parsec equal to $3.26$
light years. One parsec is equal to the distance at which $1$ AU subtends an angle
of one arcsecond (setningen er lik paa wikipedia og Barbara ryden). Most
cosmological distances are measured in parsec, abbreviated Pc, with either the
prefix mega or giga as cosmology deals mostly with intergalactic scales much
larger than the average distance between two galaxies.\\

Due to the fact that the universe is expanding, light traveling towards us from
far away objects gets redshifted. Since the expansion of the universe is
isotropic, i.e, the expansion rate of the universe is equal in all directions,
the redshift of this light can be used to measure the distance at which we
observe objects in the universe. The cosmic redshift $z$ is given by the
relation
\begin{equation}
    z = \frac{\lambda_{obs}}{\lambda_{em}} - 1,
\end{equation}
where $\lambda_{obs}$ is the observed wavelength we measure here on earth and
$\lambda_{em}$ is the wavelength emitted by the observed object. by combining this with the doppler formula
$\frac{\lambda_{obs}-\lambda_{em}}{\lambda_{em}} = \frac{v}{c}$, one gets
\begin{equation}
    z = \frac{v}{c},
\end{equation}
where $v$ is the velocity of the observed object relative to the observer. The
velocity of an object due to the expansion of the universe is given by Hubbles law
\begin{equation}
    v = H_0 d_p,
\end{equation}
where $d_p$ is what is known as proper distance and $H_0$ is the Hubble
parameter at present time. The Hubble parameter measures the expansion rate of
the universe as a function of time. (Kanskje prate om verdien av H0). Proper distance can be interpreted as the length measured if one
was to use a ruler between two objects $a$ and $b$ at a fixed time $t$ taking
into account the fact that our universe is expanding. Before introducing how proper distance is calculated it can be
useful to talk about the concept of comoving distance first. If we choose a time
$t=t_0$, which is present time, and introduce a coordinate system at this time,
we can choose an arbitrary point to center ourselves in in this coordinate
system. If we apply a ruler in this coordinate system and measure the distance
from our point to another arbitrary point, this is the comoving distance $r$.
The comoving distance remains constant through time as the universe expands. The
comoving distance is simply the distance measured in a coordinate system at
$t=t_0$ (dette er kanskje en krokete formulering $d_c$). The proper distance on the other hand factors in the expansion of the
universe through the scale factor $a(t)$. The proper distance is given as
\begin{equation}
    d_p = \int_{r_0}^{r_1}a(t)rdr,
\end{equation}
Where $a(t)$ is a quantity known as the scale factor. The scale factor parametrises the relative expansion of the
universe. It is defined in such a way that $a_0$, subscript $0$ meaning
$a(t=t_0)$ where $t_0$ is present time, is equal to one. This definition of
proper distance also implies that comoving distance is the proper distance at $t=t_0$.
With these properties in place we can now see how redshift, which is a
measurable quantity, can be used to measure the real distance to objects in our universe.\\

Another measurable measurable quantity is flux emitted from an astronomical object.
The flux $F$ is defined as
\begin{equation}
    F = \frac{L}{4\pi R^2},
\end{equation}
where $L$ is the luminosity and $R$ is the distance to the object. Luminosity is a measure
of the absolute electromagnetic power emitted from an astronomical object. The flux however, measured in $W/m^2$, is simply the luminosity
divided by the area of a shell with a radius at which we observe the object, assuming the radiation is isotropically emitted. Some astronomical phenomena, like a type 1a supernova, have a fixed luminosity
due to the characteristics of the process that creates it. This is called a standard candle. By measuring the observed flux from such a phenomena, one can then
calculate the the luminosity distance as 
\begin{equation}
    d_L = \sqrt{\frac{L}{4\pi F}}.
\end{equation}
(Kanskje si noe mer om svakheter med denne.)


\subsection{Modelling the evolution of the universe and the $\Lambda$CDM model}
The cosmological principle states that on sufficiently
large scales the universe is isotropic and homogenous. This means the universe
looks the same in every direction for every observer at any point in space for
sufficiently large scales. By sufficiently large scales one means
distances roughly $100$Mpc or larger \cite[p.~12]{ryden2017introduction}. This has been proved to be a good
assumption by surveys like the Sloan sky survey (svak formulering. Trenger ogsaa kilde). This was an important assumption
for a particular solution of the Einstein field equation. This equation, first published by
Albert Einstein in $1915$ in relation to his theory of general relativity. This equation
relates the geometry of spacetime to the distribution of matter within it.
The Friedman-Lemaître-Robertson-Walker metric, which assumes the universe is
homogenous, isotropic and expanding provides an analytical solution to the
Einstein equation (kanskje forklare hva en metrikk er for noe). Using this
metric together with the Einstein field equations results in the important
equations known as the first and second Friedman equations respectively as
\begin{equation}
    \frac{\dot{a}^2 + kc^2}{a^2} = \frac{8\pi G\rho + \Lambda c^2}{3}
\end{equation}
and
\begin{equation}
    \frac{\ddot{a}^2}{a^2} = -\frac{4\pi G}{3}(\rho + \frac{3p}{c^2}) + \frac{\Lambda c^2}{3}.
\end{equation}
(Kanskje inkludere denne utledningen fra kosmo 2 i et appendiks)
These differential equations relate evolution of the scale factor $a$ with the
following properties of the universe density $\rho$, pressure $p$, cosmological constant $\Lambda$, gravitational
constant $G$ and the speed of light $c$ (ikke glem k). For a flat universe $k=0$
(Hva er et flatt univers). In these equations $\dot{a}$ and $\ddot{a}$ dot denotes
derivative and double derivative with respect to time, meaning that $a=a(t)$ is a
function of time. \\

In modern cosmology, the dominating model for describing the universe is the $\Lambda$CDM model. Its name is an abbreviation for what 
is considered the main energy contributions governing the expansion of the universe. The cosmological constant $\Lambda$ represents dark energy, CDM is an abbrevation for cold dark matter
and lastly we have ordinary matter which is what we interact with in our everyday lives. The current estimates suggest that approximately $69\%$ of the universe consists of dark energy while the remaining $31\%$ (Dette er hentet fra Planck 2018) is attributed to dark matter at around 
$27\%$ and $4\%$ for regular matter leaving only trace contributions from other energy contributing factors such as photons ($\gamma$) and neutrinos ($\nu$).(Kanskje vaere mer spesifikk her) When modelling the universe these quantites enter into 
what is called density parameters $\Omega_i$, where $i$ represents a certain type of energy contribution to the universe i.e dark matter or CDM. The density parameter is defined as
\begin{equation}
    \Omega_{i,0} = \frac{\rho_{i,0}}{\rho_{c,0}},
\end{equation}
where $\rho_{i,0}i$ is the density of the current energy contribution and $\rho_{c,0}$ is
the critical density of the universe. Subscript zero indicating these quantities
measured at present time.The critical density is the density at
which gravity counteracts the expansion and expansion will eventually stop
(Dette er jeg veldig usikker på om er riktig å si). The critical density at a
given time is given as $\rho_c=\frac{3H(t)^2}{8\pi G}$. For a flat universe we have
\begin{equation}
    \sum_i \Omega_i = 1.
\end{equation}
A given energy density evolves through time with the scale
factor as
\begin{equation}
    \rho_i=\rho_{i,0}a^{-3(1+w_i)},
\end{equation}
where $w_i$ is a constant depending on what energy density contribution one is
considering. Summing over all energy
density contributions and using the fact that the Hubble parameter can be
expressed as $H=\frac{\dot{a}}{a}$, one can rewrite the first friedman equation
as
\begin{equation}
    H(t)=H_0\sqrt{(\Omega_{CDM,0} + \Omega_{b})a^{-3} + \Omega_{r,0}a^{-4} + \Omega_{\Lambda,0}},
\end{equation}
Where $b$ represents baryonic matter and $r$ is radiation
(Her trengs det kanskje mer forklaring og er kanskje ikke den riktige versjonen
av likningen.) 

\subsection{Growth of matter perturbations in linear perturbation theory.}
As i have previously stated, the universe is isotropic and homogenous. This
however was for sufficiently large scales at around $100$ Mpc and larger. On
smaller scales the universe is not isotropic and homogenous. A single galaxy,
for example, is denser than the intergalactic medium that separates it from other
galaxies. The anisotropy in temperature of the cosmic microwave background (CMB), as measured by
the Planck satellite and its predecessor CMB experiments, is measured to be
around $\Delta T/T=10^{-5}$. This shows that at around redshift $z=1000$, we had
relatively small fluctuations while we today observe galaxy clusters which has around
$200$ times larger density than the average density of an equal sphere in the
universe (SChneider side 342.). This suggests that the original density
perturbations grow over time. To describe this evolution, one defines the relative density
contrast as
\begin{equation}
    \delta(\vec{r}, t) \equiv \frac{\rho(\vec{r}, t) - \bar{\rho}(t)}{\bar{\rho}(t)}.
\end{equation}
Here $\bar{\rho}{t}$ denotes the mean matter density of the whole universe at
time $t$, while $\rho(\vec{r}, t)$ denotes the local density at position
$\vec{r}$ at time $t$.\\

The growth of these perturbations can, on scales substantially smaller than the
hubble radius, which is the radius of the observable universe, be described in
the framework of linear perturbation theory. On these scales Newtonian gravity
is sufficient to describe the nature of structure growth. With the approximation
that the universe only consists of pressureless matter, which is the case for
cold dark matter in which is the main constituent of the matter in the universe, we
will model the matter dsitribution as a pressureless fluid. The equations of
motion for such a fluid is given by the equations
\begin{equation}\label{eq:continuuity}
    \frac{\partial \rho}{\partial t} + \nabla\cdot(\rho \vec{v})=0,
\end{equation}
\begin{equation}\label{eq:eulereq}
    \frac{\partial \vec{v}}{\partial t} + (\vec{v}\cdot\nabla)\vec{v}=-\frac{\nabla P}{\rho}-\nabla \Phi,
\end{equation}
\begin{equation}\label{eq:poisson}
    \nabla ^2\Phi=4\pi G\rho.
\end{equation}
Here $\vec{v}=\vec{v}(\vec{r},t)$ is the velocity field of the fluid and $\Phi=\Phi(\vec{r},t)$
is the newtonian gravitational potential. $P$ is the pressure of the fluid. Equation \ref{eq:continuuity} is the
continuuity equation which tells us that the density changes with the flow of
mass. Equation \ref{eq:eulereq}, which is the Euler equation, describes the
behaviour of the fluid under the influence of an external force. The last
equation, equation \ref{eq:poisson}, is the Poisson equation which relates the
gravitational potential to the density field. These equations are solvable in
the limit $\vert\delta\vert \ll 1$, and can then be used to derive an expression
for relatively small density contrasts, which can be used to describe the
evolution of density perturbations on large scales (SJekk opp om dette er
riktig.). We can model small perturbations to first order by substituing $\rho =
\rho_0 + \delta \rho$, $\vec{v} =\vec{v_0} + \delta \vec{v}$, $\vec{v} =\vec{v_0}
+ \delta \vec{v}$, $P = P_o + \delta P$ and $\Phi = \Phi_0 +\delta\Phi$. Here
$\delta$ is used to assign small perturbations to the physical quantities and should not be
confused with the previously defined density contrast. The velocity also has to
take into account the expansion of the universe in addition to the peculiar
velocity of the particles giving $\vec{v} = H(t)\vec{r} + \vec{v}_{pec}$. After a
thorough calculation, and transforming the solution to comoving coordinates $\vec{x}$, (Kanskje sitere dette? Den er veldig lang) one will arrive at the following expression for the
evolution of the density contrast
\begin{equation}
    \frac{\partial^2 \delta}{\partial t^2} + 2H(t) \frac{d \delta}{dt}=4\pi G\bar{\rho}\delta.
\end{equation}
This equation only contains time derivatives, and all the coefficients does not
depend on $\vec{x}$. Therefore $\delta(\vec{x}, t)$ can then be expressed a spatial and time
dependent expression on the form 
\begin{equation}
    \delta(\vec{x}, t) = D(t)g(\vec{x}).
\end{equation}
Here $g(\vec{x})$ is an arbitrary function of the spatial comoving coordinate
$\vec{x}$, and $D(t)$ satisfies
\begin{equation}
    \frac{\partial^2 D}{\partial t^2} + 2H(t) \frac{d D}{dt}=4\pi G\bar{\rho}D.
\end{equation}
This equation has two solutions. One solution is strictly increasing and one is
strictly decreasing. As time evolves the decreasing solution will become negligible
and the increasing solution will dominate. This solution, denoted as $D_+(t)$,
is called the growth factor and determines the amplitude of structure growth as
a function of time. Since $t\propto a$, one can then model the growth factor for
a given cosmological model. We will lastly define the function
\begin{equation}
    f(a) \equiv \frac{d log D_+}{d log a},
\end{equation}
to quantify the relationship between the growth of structure and the expansion
of the universe.

\subsection{Two point correlation functions and the matter power spectrum.}
Matter in the universe is not randomly distributed. As a result of how structure in the universe evolves, galaxies for example,
does spread out randomly but they gather in groups or clusters. This means that given a random point in space, the probability of finding
a galaxy in the vicinity of that point is higher if the randomly chosen point by chance landed on a galaxy. The probability of finding a galaxy
at point $\vec{x}$ is not independent of wether there is a galaxy at an neighbouring point $\vec{y}$. This is a statistical property of the distribution
of galaxies and can be described by a two point correlation function. The two point correlation function for galaxies $\xi_{g}(r)$ describes the excess probability
over random for finding two galaxies separated by a distance $r$. The correlation is related to the density contrast as
\begin{equation}
    \xi(\vert\vec{r_1}-\vec{r_2}\vert)=\langle\delta(\vec{r_1})\delta(\vec{r_2})\rangle
\end{equation}
\\

The matter power spectrum $P(k)$ provides a statistical description of the
distribution of matter in the universe, but in fourier space. The matter power spectrum describes the amplitude of 
the density contrast on different length scales $L=\frac{2\pi}{k}$, where $k$ is
the fourier wave number. The matter power spectrum is related to the two point
correlation function as a fourier transform given by
\begin{equation}
    P(k)=2\pi\int_0^\infty x^2\frac{sin(kx)}{kx}\xi(x)dx.
\end{equation}
Both of these quantites are statistical properties and are of high importance
when comparing models to observations or simulations. Since we only have one
universe to sample from when doing observations, when we compare with the
predictions from models a pure image comparison may not prove to give an
accurate picture of the performance of our models(Må kanskje forklares bedre). It is therefore important to
note that two universes are considered identical if they inhabit the same
statistical properties. A simulation or calculation of structure growth may
not look identical to anything anywhere in the universe (if it is not infinately
large), but if the statistical properties are the same, then the models are
correct. This makes the power spectrum and two point correlation function
important vital tools when studying the universe.

\section{Cosmological N-body simulations}
Analytical approaches, such as linear perturbation theory has its limits.
Especially when taking into consideration small scale interactions between
galaxies. Using linear perturbation theory, gravitiational interactions on small scales can not be described in
enough detail to accurately compare observational data to theoretical
predictions. Therefore numerical simulations of structure formation is an
important tool when studying the properties of the universe. Numerical
simulations have been crucial for establishing $\Lambda CDM$ as the standard
model for cosmology as through simulations it has become possible to separate
the predictions of different models when comparing models to observations
\cite[p.~361]{schneider2006extragalactic}. The first cosmological N-body
simulation was conducted by \cite{PeeblesPJE1970SotC}, where the equations of
motion for $300$ particles were solbed to study the formation of galaxy clusters. Due to the significant increase in
computer power over the last few decades, one has been able to perform
increasingly more detailed numerical simulations where one can afford to
increase both the spatial and temporal dimension in the simulations.\\

When simulating large scale structure formation in the universe, it is mostly
sufficient to take into account dark matter, as this is the primary contributor
to the mass in the univers. The size of dark matter particles, which is still an
unobserved, and will be a lot smaller than the simulation volume. Therefore
all the dark matter particles in the simulation is represented by point
particles with mass $M$, where each point particle is a body of mass
representing multiple dark matter particles. One also has to restrict the
simulation volume of the simulation. The size of the universe is too large to
simulate in detail. It may also be infinite. Therefore one has to select a
simulation volume that is representative for the effects one wants to model. For
simulations where one wants to examine large scale structures, the simulation
volume is usually a box with size $L>200$Mpc as large scale structures are hardly present on
scales smaller than this \cite[p.~362]{schneider2006extragalactic}. Due to
restricting the simulation to a small box slice of the universe, problems with
the boundaries of the simulation volume will rise. If not treated, particles at
the boundary of the simulation volume will not be affected by gravity as many
neighbouring particles as particles in the middle of the simulations volum. To
account for the fact the universe should be isotropic and homogenous on large
scales one has to apply periodic boundary conditions. This means that the cube
is extended periodically. A particle leaving the volume in one end will reappear
in the other end. Particles will also interact gravitationally in the same
manner meaning that particles opposing sides of the simulation volume will
effect each other as if the simulation volume was extended by adding an
identical volume side by side.

\subsection{pair wise summation}
The simplest and most intuitive way of performing N-body simulations is by
modelling gravity as a force and applying Newtons second law to calculate the
acceleration vector of the particles. For every particle one calculates the
force acting on it from all other particles by summing over the contribution
from all other particles. The computational cost of this approach scales as
$N^2$, where $N$ is the number of particles in the simulation. This makes this
approach not feasible when including $N\gtrsim10^6$ particles (kanskje siter). However
interesting problems can be studied by not exceeding this number of particles. This problem can be formulated mathematically as
\begin{equation}\label{eq:newtongravacc}
    \frac{d^2r_i}{dt^2}=-G\sum_{i=1}^{N}\sum_{j\neq i}^N\frac{m_j}{\vert\vec{r_j}-\vec{r_i}\vert^3}(\vec{r_j}-\vec{r_i}),
\end{equation}
where we calculated the acceleration on the $i$th particle with position
$\vec{r}$ by summing over all particles $j\neq i$ with position $\vec{r_j}$.
When studying equation \ref{eq:newtongravacc}, one can see that if two particles
come very close numerical instability will occur in the form of a division by
zero giving an unreasonably high force calculation. To counteract this problem
force softening is introduced. A way to do this is to modify the denomenator of
equation $\ref{eq:newtongravacc}$ by including a small factor $\epsilon$
replacing $\Delta r_{i,j}=\vert\vec{r_j}-\vec{r_i}\vert$ with $\sqrt{\Delta
r_{i,j}^2+\epsilon^2}$(Siter klypin). We will now use the notation where the acceleration is
given as $a(t)=d^2r(t)/dt^2$ and the velocity $v(t)=dr(t)/dt$.
From equation \ref{eq:newtongravacc} we now have an expression for the acceleration of each object.
This gives us the the coupled system ODEs necessary to solve for the position as
\begin{equation}
    v(t)=\frac{dr(t)}{dt} \quad\mathrm{and}\quad a(t)=\frac{dv(t)}{dt}.
\end{equation}
We now discretize the time variable where $t_n$ is the time at a specific
time point where $t_n=t_0+n\Delta t t$ where $\Delta t$ is an incremental
timestep. By a Taylor expansion around $t_n$, one can approximate the time
solution at timestep $t=t_n+\Delta t$ giving
\begin{equation}
    r(t_n+\Delta t) = r(t_n) + \frac{dr(t_n)}{dt}\Delta t + \frac{1}{2}\frac{d^2r(t_n)}{dt^2}\Delta t^2 +\ldots
\end{equation}
and
\begin{equation}
    v(t_n+\Delta t) = v(t_n) + \frac{dv(t_n)}{dt}\Delta t + \frac{1}{2}\frac{d^2v(t_n)}{dt^2}\Delta t^2 +\ldots.
\end{equation}
By discreting the spatial variables in the same way as the time variable one can
write $r(t_n)=r_n$, $v(t_n)=v_n$ and $a(t_n)=a_n$ respectively for every particle.
Similariliy we have $r_{n+1}=r(t_n + \Delta t)$ etc.
By truncating the taylor expansion at the second order term one can then write
an approximate solution to our problem as
\begin{center}
\begin{itemize}
    \item $a_n=-\sum_{i=1}^{N}\sum_{j\neq i}^N\frac{Gm_j}{(\Delta
    r_{i,j}^2+\epsilon^2)^{2/3}}(\vec{r_j}-\vec{r_i})$
    \item $v_{n+1} = v_n + a_n\Delta t$
    \item $r_{n+1} = r_n + v_n\Delta t.$
\end{itemize}
\end{center}
This is done for every particle $i$ for every timestep $t+\Delta t$. This is one
of the simplest ways of this system of coupled ODEs and is called the forward
Euler method. This approach however is never used in practice as the error $\epsilon$ of
the solution, as can be seen from the taylor expansion, is of order
$\epsilon\sim\mathcal{O}(\Delta t^2)$. Another significant disadvantage with this method is
that when solving for physical systems, it will not conserve energy. A numerical
integration scheme well suited for gravitational problems where energy is
conserved is the Velocity verlet algorithm. This integration scheme is what is
known as symplectic, meaning that it conserves the energy of the system
\cite[p.~31]{holmes2007introduction}. The Velocity Verlet scheme updates its
velocity and position as
\begin{center}
    \begin{itemize}
        \item $r_{n+1} = r_n + v_n\Delta t+\frac{a_n}{2}\Delta t$
        \item $v_{n+1} = v_n + \frac{\Delta t}{2}(a_{n+1}+a_n).$
    \end{itemize}
\end{center}
This method, and its variations, that preserve energy over long time periods
makes it a preferred method of choice for astrophysical gravitational problems.
(Kanskje vise en figur som sammenlikner verlet of euler for å vise ikke konstant
energi hos euler. Kanskje bytte ut verlet integrasjon med leapfrog istedenfor.)
\subsection{Particle mesh}
Another way of simulating a box with a large volume of particles is it to apply
what is called a Particle Mesh code. This method is more computationally
effective as the number of calculations scales as $\propto N$. However most of
the information is stored on a $3D$ grid covering the whole simulation volume.
This will require more memory as the resolution of the grid increases. Poissons equation for gravity reads
\begin{equation}\label{eq:poissongrav}
    \nabla^2\Phi=4\pi G\rho.
\end{equation}
(Kanskje bruke den formen av likningen alle andre bruker med rho-rho bakgrunn)This equation can be solved for the gravitational potential $\Phi$ and updates
the force field as $\vec{F}=-\nabla\Phi$. This method is reliant on assigning a
density field to the simulation volume of individual particles as a
representation for $\rho$ in equation \ref{eq:poissongrav}. One popular method 
for this is what is known as cloud in cell (CIC) (KLypin). (Det
er her jeg har lest om det, men det er ikke de som har funnet det opp. Er det
riktig aa sitere slik da?). One wants to calculate the density $S$ at a given
distance given by the coordinate point $(x,y,z)$ from the particle. The density at a given point is given as
$S(x)S(y)S(z)$, where the cloud in cell scheme for assigning a value to $S$
reads
\begin{equation}
    S(x)=\frac{1}{\Delta x}
    \begin{cases}
        1, &\quad\text{if }\vert x \vert < \Delta x/2\\
        0, &\quad\text{otherwise},
      \end{cases}
\end{equation}
where $\Delta x$ is the cell size.

\section{Statistics}
\subsection{Bayes theorem}
When studying and comparing our observations and simulations with theory one has
to quantify the probability of our models parameter space given the data we use.
I.e what parameters will make our model compare best with our observations or
simulations. Consider the probability $P(A, B)$, which is the probability for two events $A$ and $B$ occuring.
If the two events $A$ and $B$ are independent of each other, the probability is simply given as $P(A, B) = P(A)P(B)$.
This is however not the case if the events $A$ and $B$ are dependent on each other. Take for instance the probability
of drawing two knights in a row from a deck of cards. The first draw alters number of cards in the deck and will therefore affected
the probability of the other event. We write this as $P(A,B) = P(A)P(B\vert A)$. Here $P(B\vert A)$ denotes the probability of $B$ given the fact that 
event $A$ has occured. We can likewise write $P(A,B) = P(B)P(A\vert B)$. Now lets consider the event $A$ being the parameters of our model $\theta$ and
event $B$ being our data represented by $d$. We can then write $P(\theta,d) = P(\theta)P(d\vert \theta)=P(d)P(\theta\vert d)$. Rearranging terms we will get
what is known as Bayes theorem (sitere probability in physics)
\begin{equation}\label{eq:bayes}
    P(\theta\vert d) = \frac{P(d\vert \theta)P(\theta)}{P(d)}.
\end{equation}
$P(\theta\vert d)$ is what is known as the posterior and quantifies an
important question one would ask when studying cosmolgy or physics in general. What is the probability
of the parameters of the model being correct given the data one is studying? likewise $P(d\vert\theta)$ is the
probability of getting our data given the parameters we have testet. This is what is known as
the likelihood and is usually denoted as $\mathcal{L}(\theta)$. $P(\theta)$ is what is
known as the prior. This is used to quantify what we already know about our
parameter space. $P(d)$ is what is known as the evidence and acts as a
normalization factor. \\

In most practical examples examples our model contains multiple parameters we want to fit. Lets say we have a model
dependent on the parameters $\theta_0$ and $\theta_1$. We will then get the joint probability distribution $P(\theta_0, \theta_1)$
If we want to know the probability of $\theta_0$ independently of $\theta_1$ we can get what is know as the marginal distribution given by
\begin{equation}
    P(\theta_0)=\int_{-\infty}^{\infty}P(\theta_0, \theta_1)d\theta_1
\end{equation}

\subsection{Parameter estimation}
When comparing models with data, one has to find an expression to quantify the
agreement between the model and the data. An important statistical tool is what
is called the $\chi^2$ test. Given two functions $y^{model}_i$ and $y^{data}_i$,
representing the model and data respectively where $i$ is a given data point,
the $\chi^2$ becomes
\begin{equation}
    \chi^2=\sum_{ij}(y^{model}_i-y^{data}_i)C_{ij}^{-1}(y^{model}_j-y^{data}_j),
\end{equation}
where $C_{ij}$ is the covariance matrix if the data is correlated. If the data
is uncorrelated, the $\chi^2$ takes a simpler form
\begin{equation}
    \chi^2=\sum_{i}\frac{(y^{model}_i-y^{data}_i)^2}{\sigma_i^2},
\end{equation}
where $\sigma_i$ is the error estimate for the given data point $i$. This is a
useful tool for testing the scatter of our data. The likelihood function
$\mathcal{L}(\theta)=P(d\vert \theta)$ describes the probability of our dataset
$d$ being true given our parameters. By holding our dataset constant one can
maximize the likelihood function. In other words one can try to find the most
probable value for $\theta$. If one assumes the data is given by a gaussian
distribution, the likelihood function is defined by a multivariate gaussian
\begin{equation}
    \mathcal{L}(\theta) = \frac{1}{\sqrt{(2\pi)^n\vert C \vert}}\mathrm{exp}\big[{-\frac{1}{2}}\sum_{ij}(y^{model}_i-y^{data}_i)C_{ij}^{-1}(y^{model}_j-y^{data}_j)\big].
\end{equation}
From this we can see that the likelihood and the $\chi^2$ is related as
$\mathcal{L}\propto \mathrm{exp}[-\frac{1}{2}\chi^2]$. From this it is also evident that
minimizing the likelihood is the same as minimizing $\chi^2$ (bør ikke maksimere
likelihood minimere chisquare?). Using bayes theorem given by equation
\ref{eq:bayes} one can see that we now have an expression for the likelihood.
The evidence $p(d)$, which acts as a normalization factor is not important as we
want the relative probabilites of our parameters and therefore this term can be
ignored \cite{heavens2010statistical}. If one chooses a flat prior, one where
$p(\theta)$ is constant, one simply gets by adressing bayes theorem
\begin{equation}
    P(\theta\vert d) \propto \mathcal{L}(\theta).
\end{equation}
\subsection{Sampling and Metropolis hastings MCMC}
When dealing with distributions $P(\vec{\theta},\vec{d})$, where there is a
large amount of parameters $\vec{\theta}=(\theta_0, \theta_1,\dots, \theta_n)$,
a simple gridding algorithm can prove to be both time and memory consuming. If
we also take into account the fact that one may deal with gaussian
distributions, there are large areas of parameter space where the probability of
that particular configuration of parameters being true is slim to none.
Therefore by gridding one spends a significant amount of time exploring a relatively
uninteresting part of parameter space. Therefore it is useful to introduce the
concept of sampling and the Metropolis-hastings Markov-Chain Monte Carlo
method.\\

The Monte Carlo principle states that if you take
independent and identically distributed samples $\theta$ from an arbitrary
function $f(\theta)$, as the number of samples increases, the distribution of
samples will converge towards the function $f(\theta)$. This is expressed
mathematically as
\begin{align}
    P_N(\theta) &= \frac{1}{N}\sum_{i=1}^N\delta(\theta)\\
    &\rightarrow \lim_{n\to\infty}P_N(\theta)=p(\theta)
\end{align}
There is however a problem in that the function we want to calculate
$P(\theta\vert d)$ is a distribution that is unknown to us.(sjekk opp dette). We
know how to calculate it, but we cannot draw from it. Therefore one introduces a
proposal distribution $q(\theta)$. This is a known function that one can easily
evualuate and pick samples from.

